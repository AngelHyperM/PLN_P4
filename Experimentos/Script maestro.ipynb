{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e92cd23e",
   "metadata": {},
   "source": [
    "**Vectorizaciones para TODAS las variantes preprocesadas**\n",
    "- Split único, fijo (random_state=0), reutilizado por variante\n",
    "- Salida: PKL por (variante, representación)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0092cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from joblib import dump\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c26e7b",
   "metadata": {},
   "source": [
    "**CONFIG GENERAL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92fef990",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(r\"C:\\Users\\Miner\\OneDrive\\Documentos\\7to semestre\\Procesamiento de Lenguaje Natural\\Practice IV - Sentiment Analysis\\PLN_P4\\Archivos_Procesados\")\n",
    "\n",
    "OUT_DIR  = Path(r\"C:\\Users\\Miner\\OneDrive\\Documentos\\7to semestre\\Procesamiento de Lenguaje Natural\\Practice IV - Sentiment Analysis\\PLN_P4\\artifacts_all\")\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e5db4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_COL_TITLE = \"title\"\n",
    "TEXT_COL_OPIN  = \"opinion\"\n",
    "TARGET_COL     = \"polarity\"\n",
    "DOCID_COL      = \"doc_id\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00e7067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "XLSX_FILES = [\n",
    "    \"Rest_Mex_2022_preprocessed__all_on.xlsx\",\n",
    "    \"Rest_Mex_2022_preprocessed__minimal.xlsx\",\n",
    "    \"Rest_Mex_2022_preprocessed__no_accents.xlsx\",\n",
    "    \"Rest_Mex_2022_preprocessed__no_emoji_feats.xlsx\",\n",
    "    \"Rest_Mex_2022_preprocessed__no_en2es.xlsx\",\n",
    "    \"Rest_Mex_2022_preprocessed__no_lemma.xlsx\",\n",
    "    \"Rest_Mex_2022_preprocessed__no_negjoin.xlsx\",\n",
    "    \"Rest_Mex_2022_preprocessed__no_sel.xlsx\",\n",
    "    \"Rest_Mex_2022_preprocessed__no_stopwords.xlsx\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cba194",
   "metadata": {},
   "source": [
    "**Parámetros de split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47ec9945",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.2\n",
    "SEED = 0\n",
    "SPLIT_IDS_PATH = OUT_DIR / \"split_ids.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e547eef2",
   "metadata": {},
   "source": [
    "**UTILIDADES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97e178ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_xlsx_text_target(path: Path):\n",
    "    df = pd.read_excel(path)\n",
    "    # Normalizar nombres:\n",
    "    cols_l = {c.lower().strip(): c for c in df.columns}\n",
    "    tcol = cols_l.get(TEXT_COL_TITLE.lower(), TEXT_COL_TITLE)\n",
    "    ocol = cols_l.get(TEXT_COL_OPIN.lower(), TEXT_COL_OPIN)\n",
    "    ycol = cols_l.get(TARGET_COL.lower(), TARGET_COL)\n",
    "\n",
    "    # doc_id si no existe\n",
    "    if DOCID_COL not in df.columns:\n",
    "        df[DOCID_COL] = np.arange(len(df), dtype=int)\n",
    "\n",
    "    # texto = titulo + opinion\n",
    "    df[\"text\"] = (df[tcol].fillna(\"\").astype(str) + \" \" +\n",
    "                  df[ocol].fillna(\"\").astype(str)).str.strip()\n",
    "\n",
    "    y = df[ycol].astype(int).values\n",
    "    return df[[DOCID_COL, \"text\"]], y\n",
    "\n",
    "def make_or_load_split_ids(df_ids_text: pd.DataFrame, y: np.ndarray):\n",
    "    \"\"\"\n",
    "    Crea o carga ids de train/test para usar el MISMO split en todas las variantes.\n",
    "    Compatible con pandas 2.0+ (sin .append).\n",
    "    \"\"\"\n",
    "    if SPLIT_IDS_PATH.exists():\n",
    "        ids = pd.read_parquet(SPLIT_IDS_PATH)\n",
    "        train_ids = set(ids.loc[ids[\"split\"] == \"train\", DOCID_COL].tolist())\n",
    "        test_ids  = set(ids.loc[ids[\"split\"] == \"test\",  DOCID_COL].tolist())\n",
    "        return train_ids, test_ids\n",
    "\n",
    "    # Split estratificado y reproducible\n",
    "    tr_ids, te_ids = train_test_split(\n",
    "        df_ids_text[DOCID_COL].values,\n",
    "        test_size=TEST_SIZE,\n",
    "        random_state=SEED,\n",
    "        shuffle=True,\n",
    "        stratify=y\n",
    "    )\n",
    "\n",
    "    # >>> reemplazo de .append por pd.concat <<<\n",
    "    ids = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame({DOCID_COL: tr_ids, \"split\": \"train\"}),\n",
    "            pd.DataFrame({DOCID_COL: te_ids, \"split\": \"test\"})\n",
    "        ],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    ids.to_parquet(SPLIT_IDS_PATH, index=False)  # requiere pyarrow o fastparquet\n",
    "    return set(tr_ids), set(te_ids)\n",
    " \n",
    "\n",
    "def get_texts_by_ids(df_ids_text: pd.DataFrame, train_ids: set, test_ids: set):\n",
    "    tr = df_ids_text[df_ids_text[DOCID_COL].isin(train_ids)].sort_values(DOCID_COL)\n",
    "    te = df_ids_text[df_ids_text[DOCID_COL].isin(test_ids)].sort_values(DOCID_COL)\n",
    "    return tr[\"text\"].tolist(), te[\"text\"].tolist(), tr[DOCID_COL].tolist(), te[DOCID_COL].tolist()\n",
    "\n",
    "def get_targets_by_ids(full_df_path: Path, train_ids: list, test_ids: list):\n",
    "    df = pd.read_excel(full_df_path)\n",
    "    cols_l = {c.lower().strip(): c for c in df.columns}\n",
    "    ycol = cols_l.get(TARGET_COL.lower(), TARGET_COL)\n",
    "    if DOCID_COL not in df.columns:\n",
    "        df[DOCID_COL] = np.arange(len(df), dtype=int)\n",
    "    df = df[[DOCID_COL, ycol]]\n",
    "    df_train = df.set_index(DOCID_COL).loc[train_ids]\n",
    "    df_test  = df.set_index(DOCID_COL).loc[test_ids]\n",
    "    return df_train[ycol].astype(int).values, df_test[ycol].astype(int).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b67237e",
   "metadata": {},
   "source": [
    "**Vectorizadores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b934eecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_binary(train_texts, test_texts, ngram=(1,1), min_df=1, max_features=None, lowercase=True):\n",
    "    v = CountVectorizer(binary=True, ngram_range=ngram, min_df=min_df,\n",
    "                        max_features=max_features, lowercase=lowercase)\n",
    "    Xtr = v.fit_transform(train_texts); Xte = v.transform(test_texts)\n",
    "    return Xtr, Xte, v\n",
    "\n",
    "def vec_count(train_texts, test_texts, ngram=(1,2), min_df=3, max_features=None, lowercase=True):\n",
    "    v = CountVectorizer(binary=False, ngram_range=ngram, min_df=min_df,\n",
    "                        max_features=max_features, lowercase=lowercase)\n",
    "    Xtr = v.fit_transform(train_texts); Xte = v.transform(test_texts)\n",
    "    return Xtr, Xte, v\n",
    "\n",
    "def vec_tfidf(train_texts, test_texts, ngram=(1,2), min_df=2, max_features=50000, lowercase=True):\n",
    "    v = TfidfVectorizer(ngram_range=ngram, min_df=min_df,\n",
    "                        max_features=max_features, lowercase=lowercase,\n",
    "                        use_idf=True, sublinear_tf=True)\n",
    "    Xtr = v.fit_transform(train_texts); Xte = v.transform(test_texts)\n",
    "    return Xtr, Xte, v\n",
    "\n",
    "def save_artifacts(Xtr, Xte, ytr, yte, vec, out_dir: Path, tag: str):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    dump(Xtr, out_dir / f\"X_train_{tag}.pkl\")\n",
    "    dump(Xte, out_dir / f\"X_test_{tag}.pkl\")\n",
    "    dump(ytr, out_dir / f\"y_train.pkl\")\n",
    "    dump(yte, out_dir / f\"y_test.pkl\")\n",
    "    dump(vec, out_dir / f\"vectorizer_{tag}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e5fb4f",
   "metadata": {},
   "source": [
    "**FLUJO PRINCIPAL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea62ff20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split fijo -> train: 24169  test: 6043\n",
      "\n",
      "=== Variante: all_on ===\n",
      "  Binaria: (24169, 45690) (6043, 45690) Vocab: 45690\n",
      "  Frecuencia: (24169, 118054) (6043, 118054) Vocab: 118054\n",
      "  TF-IDF: (24169, 50000) (6043, 50000) Vocab: 50000\n",
      "\n",
      "=== Variante: minimal ===\n",
      "  Binaria: (24169, 45690) (6043, 45690) Vocab: 45690\n",
      "  Frecuencia: (24169, 118054) (6043, 118054) Vocab: 118054\n",
      "  TF-IDF: (24169, 50000) (6043, 50000) Vocab: 50000\n",
      "\n",
      "=== Variante: no_accents ===\n",
      "  Binaria: (24169, 45690) (6043, 45690) Vocab: 45690\n",
      "  Frecuencia: (24169, 118054) (6043, 118054) Vocab: 118054\n",
      "  TF-IDF: (24169, 50000) (6043, 50000) Vocab: 50000\n",
      "\n",
      "=== Variante: no_emoji_feats ===\n",
      "  Binaria: (24169, 45690) (6043, 45690) Vocab: 45690\n",
      "  Frecuencia: (24169, 118054) (6043, 118054) Vocab: 118054\n",
      "  TF-IDF: (24169, 50000) (6043, 50000) Vocab: 50000\n",
      "\n",
      "=== Variante: no_en2es ===\n",
      "  Binaria: (24169, 45690) (6043, 45690) Vocab: 45690\n",
      "  Frecuencia: (24169, 118054) (6043, 118054) Vocab: 118054\n",
      "  TF-IDF: (24169, 50000) (6043, 50000) Vocab: 50000\n",
      "\n",
      "=== Variante: no_lemma ===\n",
      "  Binaria: (24169, 45690) (6043, 45690) Vocab: 45690\n",
      "  Frecuencia: (24169, 118054) (6043, 118054) Vocab: 118054\n",
      "  TF-IDF: (24169, 50000) (6043, 50000) Vocab: 50000\n",
      "\n",
      "=== Variante: no_negjoin ===\n",
      "  Binaria: (24169, 45690) (6043, 45690) Vocab: 45690\n",
      "  Frecuencia: (24169, 118054) (6043, 118054) Vocab: 118054\n",
      "  TF-IDF: (24169, 50000) (6043, 50000) Vocab: 50000\n",
      "\n",
      "=== Variante: no_sel ===\n",
      "  Binaria: (24169, 45690) (6043, 45690) Vocab: 45690\n",
      "  Frecuencia: (24169, 118054) (6043, 118054) Vocab: 118054\n",
      "  TF-IDF: (24169, 50000) (6043, 50000) Vocab: 50000\n",
      "\n",
      "=== Variante: no_stopwords ===\n",
      "  Binaria: (24169, 45690) (6043, 45690) Vocab: 45690\n",
      "  Frecuencia: (24169, 118054) (6043, 118054) Vocab: 118054\n",
      "  TF-IDF: (24169, 50000) (6043, 50000) Vocab: 50000\n"
     ]
    }
   ],
   "source": [
    "# 1) Tomamos una variante \"ancla\" para crear el split (p.ej., minimal)\n",
    "anchor_path = DATA_DIR / \"Rest_Mex_2022_preprocessed__minimal.xlsx\"\n",
    "df_anchor, y_anchor = load_xlsx_text_target(anchor_path)\n",
    "train_ids, test_ids = make_or_load_split_ids(df_anchor, y_anchor)\n",
    "\n",
    "print(f\"Split fijo -> train: {len(train_ids)}  test: {len(test_ids)}\")\n",
    "\n",
    "# 2) Recorremos todas las variantes y generamos las 3 representaciones\n",
    "for fname in XLSX_FILES:\n",
    "    fpath = DATA_DIR / fname\n",
    "    var_name = fname.replace(\"Rest_Mex_2022_preprocessed__\", \"\").replace(\".xlsx\", \"\")\n",
    "    print(f\"\\n=== Variante: {var_name} ===\")\n",
    "\n",
    "    # cargar textos + y por ids del split\n",
    "    df_ids_text, _y_dummy = load_xlsx_text_target(fpath)\n",
    "    tr_texts, te_texts, tr_ids_sorted, te_ids_sorted = get_texts_by_ids(df_ids_text, train_ids, test_ids)\n",
    "    ytr, yte = get_targets_by_ids(fpath, tr_ids_sorted, te_ids_sorted)\n",
    "\n",
    "    # ---- BINARIA (unigramas) ----\n",
    "    Xtr, Xte, vec = vec_binary(tr_texts, te_texts, ngram=(1,1), min_df=1)\n",
    "    save_artifacts(Xtr, Xte, ytr, yte, vec, OUT_DIR / var_name / \"binary\", tag=\"binary\")\n",
    "    print(\"  Binaria:\", Xtr.shape, Xte.shape, \"Vocab:\", len(vec.vocabulary_))\n",
    "\n",
    "    # ---- FRECUENCIA (1-2gram, min_df=3) ----\n",
    "    Xtr, Xte, vec = vec_count(tr_texts, te_texts, ngram=(1,2), min_df=3)\n",
    "    save_artifacts(Xtr, Xte, ytr, yte, vec, OUT_DIR / var_name / \"count\", tag=\"count12_min3\")\n",
    "    print(\"  Frecuencia:\", Xtr.shape, Xte.shape, \"Vocab:\", len(vec.vocabulary_))\n",
    "\n",
    "    # ---- TF-IDF (1-2gram, max_features=50k) ----\n",
    "    Xtr, Xte, vec = vec_tfidf(tr_texts, te_texts, ngram=(1,2), min_df=2, max_features=50000)\n",
    "    save_artifacts(Xtr, Xte, ytr, yte, vec, OUT_DIR / var_name / \"tfidf\", tag=\"tfidf12_50k\")\n",
    "    print(\"  TF-IDF:\", Xtr.shape, Xte.shape, \"Vocab:\", len(vec.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4acbc7",
   "metadata": {},
   "source": [
    "**Extras (Léxicos + Emojis/Emoticonos) y concatenación**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6bf04bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from joblib import load, dump\n",
    "from scipy.sparse import csr_matrix, hstack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedd8a97",
   "metadata": {},
   "source": [
    "**Parámetros del experimento de extra**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd6af542",
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIANT_NAME = \"all_on\"          # p.ej.: \"all_on\", \"no_emoji_feats\", \"no_lemma\", ...\n",
    "BASE_REPR    = \"tfidf\"           # \"binary\" | \"count\" | \"tfidf\"\n",
    "BASE_TAGS = {                    # tags de archivo que usaste al guardar\n",
    "    \"binary\": \"binary\",\n",
    "    \"count\":  \"count12_min3\",\n",
    "    \"tfidf\":  \"tfidf12_50k\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7010c16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = OUT_DIR / VARIANT_NAME / BASE_REPR\n",
    "OUT_EXTRAS_DIR = OUT_DIR / VARIANT_NAME / f\"{BASE_REPR}_extras\"\n",
    "OUT_EXTRAS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8caa2cf",
   "metadata": {},
   "source": [
    "**Cargar split e insumos de texto**  \n",
    "Reutilizamos tus utilidades y constantes: DATA_DIR, TEXT_COL_TITLE, TEXT_COL_OPIN, TARGET_COL, DOCID_COL, SPLIT_IDS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a44686e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_path = DATA_DIR / f\"Rest_Mex_2022_preprocessed__{VARIANT_NAME}.xlsx\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb969f6",
   "metadata": {},
   "source": [
    "**Cargamos textos + y para alinear**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a32c56ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids_text, _y_all = load_xlsx_text_target(xlsx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67b98b7",
   "metadata": {},
   "source": [
    "**Cargamos ids del split fijo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50f120e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = pd.read_parquet(SPLIT_IDS_PATH)\n",
    "train_ids = ids.loc[ids[\"split\"]==\"train\", DOCID_COL].tolist()\n",
    "test_ids  = ids.loc[ids[\"split\"]==\"test\",  DOCID_COL].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d333a61d",
   "metadata": {},
   "source": [
    "**Alineamos textos en el orden del split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cff1d651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _align_texts(df_ids_text, ids):\n",
    "    dd = df_ids_text.set_index(DOCID_COL).loc[ids]\n",
    "    return dd[\"text\"].astype(str).tolist()\n",
    "\n",
    "X_train_text = _align_texts(df_ids_text, train_ids)\n",
    "X_test_text  = _align_texts(df_ids_text, test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32a0a61",
   "metadata": {},
   "source": [
    "**Cargar matrices base (sparse) y vectorizador**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80e5c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG = BASE_TAGS[BASE_REPR]\n",
    "X_train_base = load(BASE_DIR / f\"X_train_{TAG}.pkl\")\n",
    "X_test_base  = load(BASE_DIR / f\"X_test_{TAG}.pkl\")\n",
    "y_train      = load(BASE_DIR / \"y_train.pkl\")\n",
    "y_test       = load(BASE_DIR / \"y_test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c175429d",
   "metadata": {},
   "source": [
    "**Construcción de FEATURES EXTRA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25398329",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\n",
    "    r\"[\"\n",
    "    r\"\\U0001F300-\\U0001F5FF\"  # símbolos y pictogramas\n",
    "    r\"\\U0001F600-\\U0001F64F\"  # emoticonos\n",
    "    r\"\\U0001F680-\\U0001F6FF\"  # transporte y mapas\n",
    "    r\"\\U0001F700-\\U0001F77F\"\n",
    "    r\"\\U0001F780-\\U0001F7FF\"\n",
    "    r\"\\U0001F800-\\U0001F8FF\"\n",
    "    r\"\\U0001F900-\\U0001F9FF\"\n",
    "    r\"\\U0001FA00-\\U0001FAFF\"\n",
    "    r\"\\u2600-\\u26FF\"\n",
    "    r\"\\u2700-\\u27BF\"\n",
    "    r\"]\", flags=re.UNICODE\n",
    ")\n",
    "\n",
    "def extract_emojis(text):\n",
    "    return emoji_pattern.findall(text) if isinstance(text, str) else []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a50b55",
   "metadata": {},
   "source": [
    "**Clasificación de emojis por polaridad APRENDIDA desde TRAIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9bfd16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etiquetas: 1= muy negativo, 2= negativo, 3= neutro, 4= positivo, 5= muy positivo\n",
    "POS_LABELS = {4, 5}\n",
    "NEG_LABELS = {1, 2}\n",
    "\n",
    "\n",
    "y_train_ordered, y_test_ordered = get_targets_by_ids(xlsx_path, train_ids, test_ids)  \n",
    "\n",
    "pos_counts, neg_counts = {}, {}\n",
    "for txt, label in zip(X_train_text, y_train_ordered):\n",
    "    emojis = extract_emojis(txt)\n",
    "    if not emojis:\n",
    "        continue\n",
    "    if int(label) in POS_LABELS:\n",
    "        for e in emojis:\n",
    "            pos_counts[e] = pos_counts.get(e, 0) + 1\n",
    "    elif int(label) in NEG_LABELS:\n",
    "        for e in emojis:\n",
    "            neg_counts[e] = neg_counts.get(e, 0) + 1\n",
    "\n",
    "positive_emojis = {e for e in pos_counts if pos_counts[e] > neg_counts.get(e, 0)}\n",
    "negative_emojis = {e for e in neg_counts if neg_counts[e] > pos_counts.get(e, 0)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97ad4ab",
   "metadata": {},
   "source": [
    "**Emoticonos comunes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee21deef",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_emoticons = {\":)\", \":-)\", \":D\", \"(:\", \"=)\", \";)\", \";-)\", \":')\"}\n",
    "negative_emoticons = {\":(\", \":-(\", \"):\", \"='(\", \":'(\", \"D:\", \">:(\", \":-/\"}\n",
    "\n",
    "def emoji_emoticon_features(text):\n",
    "    \"\"\"\n",
    "    Devuelve: (emoji_pos+emo_pos, emoji_neg+emo_neg, exclamaciones)\n",
    "    \"\"\"\n",
    "    t = str(text)\n",
    "    # emojis\n",
    "    any_emojis = extract_emojis(t)\n",
    "    pos_e = sum(ch in positive_emojis for ch in any_emojis)\n",
    "    neg_e = sum(ch in negative_emojis for ch in any_emojis)\n",
    "    # emoticonos\n",
    "    pos_em = sum(t.count(emo) for emo in positive_emoticons)\n",
    "    neg_em = sum(t.count(emo) for emo in negative_emoticons)\n",
    "    # intensidad\n",
    "    exclam = t.count(\"!\")\n",
    "    return (pos_e + pos_em, neg_e + neg_em, exclam)\n",
    "\n",
    "def build_emoji_features(text_list):\n",
    "    feats = [emoji_emoticon_features(t) for t in text_list]\n",
    "    return np.asarray(feats, dtype=np.float32)  # shape: (n_docs, 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311c6d29",
   "metadata": {},
   "source": [
    "**Léxicos simples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1b818cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[all_on | tfidf] Base: (24169, 50000) → Con extras: (24169, 50005)\n",
      "Extras guardados en: C:\\Users\\Miner\\OneDrive\\Documentos\\7to semestre\\Procesamiento de Lenguaje Natural\\Practice IV - Sentiment Analysis\\PLN_P4\\artifacts_all\\all_on\\tfidf_extras\n",
      "Meta: {'variant': 'all_on', 'base_repr': 'tfidf', 'base_tag': 'tfidf12_50k', 'extra_columns': ['lex_pos', 'lex_neg', 'emoji_pos', 'emoji_neg', 'exclam'], 'train_shape': [24169, 50005], 'test_shape': [6043, 50005], 'notes': 'Emojis positivos/negativos aprendidos desde TRAIN según polaridad (1-5).'}\n"
     ]
    }
   ],
   "source": [
    "positive_lexicon = {\n",
    "    \"bueno\",\"excelente\",\"perfecto\",\"maravilloso\",\"increíble\",\"bonito\",\"agradable\",\n",
    "    \"recomendado\",\"limpio\",\"rápido\",\"delicioso\",\"rico\",\"amable\",\"fantástico\",\"genial\"\n",
    "}\n",
    "negative_lexicon = {\n",
    "    \"malo\",\"terrible\",\"horrible\",\"pésimo\",\"lento\",\"sucio\",\"caro\",\n",
    "    \"decepcionante\",\"desagradable\",\"asqueroso\",\"frío\",\"tardado\",\"estresante\",\"feo\"\n",
    "}\n",
    "\n",
    "def count_sentiment_words(text):\n",
    "    toks = str(text).lower().split()\n",
    "    pos = sum(t in positive_lexicon for t in toks)\n",
    "    neg = sum(t in negative_lexicon for t in toks)\n",
    "    return (pos, neg)\n",
    "\n",
    "def build_lexicon_features(text_list):\n",
    "    feats = [count_sentiment_words(t) for t in text_list]\n",
    "    return np.asarray(feats, dtype=np.float32)  # shape: (n_docs, 2)\n",
    "\n",
    "# 1) Construir extras para TRAIN/TEST\n",
    "lex_train = build_lexicon_features(X_train_text)  # (n,2)\n",
    "lex_test  = build_lexicon_features(X_test_text)   # (m,2)\n",
    "emo_train = build_emoji_features(X_train_text)    # (n,3)\n",
    "emo_test  = build_emoji_features(X_test_text)     # (m,3)\n",
    "\n",
    "# 2) Concatenar extras a la matriz dispersa base\n",
    "extra_train = np.hstack([lex_train, emo_train])   # (n, 5) -> [lex_pos, lex_neg, emoji_pos, emoji_neg, exclam]\n",
    "extra_test  = np.hstack([lex_test,  emo_test])    # (m, 5)\n",
    "\n",
    "X_train_ext = hstack([X_train_base, csr_matrix(extra_train)], format=\"csr\")\n",
    "X_test_ext  = hstack([X_test_base,  csr_matrix(extra_test)],  format=\"csr\")\n",
    "\n",
    "print(f\"[{VARIANT_NAME} | {BASE_REPR}] Base:\", X_train_base.shape, \"→ Con extras:\", X_train_ext.shape)\n",
    "\n",
    "# 3) Guardar artefactos extendidos\n",
    "dump(X_train_ext, OUT_EXTRAS_DIR / f\"X_train_{TAG}_extras.pkl\")\n",
    "dump(X_test_ext,  OUT_EXTRAS_DIR / f\"X_test_{TAG}_extras.pkl\")\n",
    "dump(y_train,     OUT_EXTRAS_DIR / \"y_train.pkl\")\n",
    "dump(y_test,      OUT_EXTRAS_DIR / \"y_test.pkl\")\n",
    "\n",
    "meta = {\n",
    "    \"variant\": VARIANT_NAME,\n",
    "    \"base_repr\": BASE_REPR,\n",
    "    \"base_tag\": TAG,\n",
    "    \"extra_columns\": [\"lex_pos\",\"lex_neg\",\"emoji_pos\",\"emoji_neg\",\"exclam\"],\n",
    "    \"train_shape\": list(X_train_ext.shape),\n",
    "    \"test_shape\":  list(X_test_ext.shape),\n",
    "    \"notes\": \"Emojis positivos/negativos aprendidos desde TRAIN según polaridad (1-5).\"\n",
    "}\n",
    "with open(OUT_EXTRAS_DIR / \"meta_extras.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Extras guardados en:\", OUT_EXTRAS_DIR)\n",
    "print(\"Meta:\", meta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tf310)",
   "language": "python",
   "name": "tf310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
