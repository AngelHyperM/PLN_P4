{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0db3ca84",
   "metadata": {},
   "source": [
    "# Practice IV - Sentiment Analysis\n",
    "## **1ra Parte Corpus preprocessing David**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f176ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Miner\\miniconda3\\envs\\tf310\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re, unicodedata, pandas as pd, os, spacy\n",
    "from spacy.lang.es.stop_words import STOP_WORDS as SPACY_STOP_ES\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28945a9d",
   "metadata": {},
   "source": [
    "Cargar Rest_Mex_2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3988e920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Opinion</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Attraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PÃ©simo lugar</td>\n",
       "      <td>Piensen dos veces antes de ir a este hotel, te...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No vayas a lugar de Eddie</td>\n",
       "      <td>Cuatro de nosotros fuimos recientemente a Eddi...</td>\n",
       "      <td>1</td>\n",
       "      <td>Restaurant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mala relaciÃ³n calidad-precio</td>\n",
       "      <td>seguirÃ© corta y simple: limpieza\\n- bad. Tengo...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MinusvÃ¡lido? Â¡No te alojes aquÃ­!</td>\n",
       "      <td>Al reservar un hotel con multipropiedad Mayan ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Es una porqueria no pierdan su tiempo</td>\n",
       "      <td>No pierdan su tiempo ni dinero, venimos porque...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>El peor huevos Benedict jamÃ¡s</td>\n",
       "      <td>Hoy tenÃ­amos el desayuno por segunda vez en po...</td>\n",
       "      <td>1</td>\n",
       "      <td>Restaurant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Definitivamente no volverÃ­a a hospedarme en Sa...</td>\n",
       "      <td>El hotel en si no es malo, pero mi experiencia...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Terrible</td>\n",
       "      <td>No estoy seguro de por quÃ© este restaurante ti...</td>\n",
       "      <td>1</td>\n",
       "      <td>Restaurant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bebidas ADULTERADAS, FATAL mi experiencia!!!</td>\n",
       "      <td>LleguÃ© a este hotel â€œpor desgraciaâ€ mi reserva...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hotel ha ido cuesta abajo</td>\n",
       "      <td>Hemos estado viniendo a Villa la Estancia dura...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0                                       PÃ©simo lugar   \n",
       "1                          No vayas a lugar de Eddie   \n",
       "2                       Mala relaciÃ³n calidad-precio   \n",
       "3                   MinusvÃ¡lido? Â¡No te alojes aquÃ­!   \n",
       "4              Es una porqueria no pierdan su tiempo   \n",
       "5                      El peor huevos Benedict jamÃ¡s   \n",
       "6  Definitivamente no volverÃ­a a hospedarme en Sa...   \n",
       "7                                           Terrible   \n",
       "8       Bebidas ADULTERADAS, FATAL mi experiencia!!!   \n",
       "9                          Hotel ha ido cuesta abajo   \n",
       "\n",
       "                                             Opinion  Polarity  Attraction  \n",
       "0  Piensen dos veces antes de ir a este hotel, te...         1       Hotel  \n",
       "1  Cuatro de nosotros fuimos recientemente a Eddi...         1  Restaurant  \n",
       "2  seguirÃ© corta y simple: limpieza\\n- bad. Tengo...         1       Hotel  \n",
       "3  Al reservar un hotel con multipropiedad Mayan ...         1       Hotel  \n",
       "4  No pierdan su tiempo ni dinero, venimos porque...         1       Hotel  \n",
       "5  Hoy tenÃ­amos el desayuno por segunda vez en po...         1  Restaurant  \n",
       "6  El hotel en si no es malo, pero mi experiencia...         1       Hotel  \n",
       "7  No estoy seguro de por quÃ© este restaurante ti...         1  Restaurant  \n",
       "8  LleguÃ© a este hotel â€œpor desgraciaâ€ mi reserva...         1       Hotel  \n",
       "9  Hemos estado viniendo a Villa la Estancia dura...         1       Hotel  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar corpus .xlsx\n",
    "df = pd.read_excel('Recursos Profe/Rest_Mex_2022.xlsx', engine='openpyxl')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "223e919b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30212 entries, 0 to 30211\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Title       30210 non-null  object\n",
      " 1   Opinion     30210 non-null  object\n",
      " 2   Polarity    30212 non-null  int64 \n",
      " 3   Attraction  30212 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 944.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58b0522a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30212, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7d6a99",
   "metadata": {},
   "source": [
    "Concatenar Title + Opinion como texto de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a29eb371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir las columnas 'Title' y 'Opinion' a tipo string.\n",
    "df['Title'] = df['Title'].astype(str)\n",
    "df['Opinion'] = df['Opinion'].astype(str)\n",
    "\n",
    "# Concatenar Title + Opinion como texto de entrada.\n",
    "df['text'] = df['Title'] + ' ' + df['Opinion']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1fb066",
   "metadata": {},
   "source": [
    "Dejar Polarity como etiqueta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd60d070",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = df['Polarity']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4a8eb1",
   "metadata": {},
   "source": [
    "NormalizaciÃ³n base:\n",
    "- MinÃºsculas, quitar signos raros, espacios extra, etc.\n",
    "- Manejo bÃ¡sico de stopwords.\n",
    "- Manejo de negaciones.\n",
    "- Manejo de repeticiones.\n",
    "- CorrecciÃ³n bÃ¡sica de errores comunes.\n",
    "\n",
    "Se aÃ±ade:\n",
    "- Emojis lexicon\n",
    "- SEL_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83620cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear diccionario de traducciÃ³n inglÃ©s -> espaÃ±ol para tÃ©rminos comunes\n",
    "en2es = {\n",
    "    \"front desk\": \"recepcion\",\n",
    "    \"check-in\": \"check_in\",\n",
    "    \"check in\": \"check_in\",\n",
    "    \"early check-in\": \"check_in_temprano\",\n",
    "    \"late check-out\": \"check_out_tarde\",\n",
    "    \"check-out\": \"check_out\",\n",
    "    \"check out\": \"check_out\",\n",
    "    \"housekeeping\": \"limpieza_cuarto\",\n",
    "    \"room service\": \"servicio_cuarto\",\n",
    "    \"customer service\": \"servicio_cliente\",\n",
    "    \"concierge\": \"conserje\",\n",
    "    \"bellboy\": \"botones\",\n",
    "    \"valet parking\": \"valet_parking\",\n",
    "    \"self parking\": \"estacionamiento_autoservicio\",\n",
    "    \"parking lot\": \"estacionamiento\",\n",
    "    \"booking\": \"reserva\",\n",
    "    \"overbooking\": \"sobre_reserva\",\n",
    "    \"reservation\": \"reserva\",\n",
    "    \"upgrade\": \"mejora_habitacion\",\n",
    "    \"downgrade\": \"degradacion_habitacion\",\n",
    "    \"no-show\": \"no_show\",\n",
    "    \"walk-in\": \"llegada_sin_reserva\",\n",
    "    \"timeshare\": \"tiempos_compartidos\",\n",
    "    \"all inclusive\": \"todo_incluido\",\n",
    "    \"half board\": \"media_pension\",\n",
    "    \"full board\": \"pension_completa\",\n",
    "    \"resort fee\": \"cargo_resort\",\n",
    "    \"resort\": \"resort\",\n",
    "    \"security deposit\": \"deposito_garantia\",\n",
    "    \"refund\": \"reembolso\",\n",
    "    \"chargeback\": \"contracargo\",\n",
    "\n",
    "    # --- HabitaciÃ³n / instalaciones ---\n",
    "    \"king bed\": \"cama_king\",\n",
    "    \"queen bed\": \"cama_queen\",\n",
    "    \"twin beds\": \"camas_individuales\",\n",
    "    \"double bed\": \"cama_matrimonial\",\n",
    "    \"sofa bed\": \"sofa_cama\",\n",
    "    \"bunk bed\": \"literas\",\n",
    "    \"extra bed\": \"cama_extra\",\n",
    "    \"rollaway bed\": \"cama_plegable\",\n",
    "    \"crib\": \"cuna\",\n",
    "    \"mattress\": \"colchon\",\n",
    "    \"pillow\": \"almohada\",\n",
    "    \"blanket\": \"cobija\",\n",
    "    \"linens\": \"sabanas\",\n",
    "    \"towels\": \"toallas\",\n",
    "    \"toilet paper\": \"papel_higienico\",\n",
    "    \"bathroom\": \"bano\",\n",
    "    \"bathtub\": \"banera\",\n",
    "    \"hot tub\": \"jacuzzi\",\n",
    "    \"jacuzzi\": \"jacuzzi\",\n",
    "    \"shower\": \"ducha\",\n",
    "    \"rain shower\": \"ducha_lluvia\",\n",
    "    \"water pressure\": \"presion_agua\",\n",
    "    \"air conditioning\": \"aire_acondicionado\",\n",
    "    \"heater\": \"calefaccion\",\n",
    "    \"safe box\": \"caja_fuerte\",\n",
    "    \"mini bar\": \"minibar\",\n",
    "    \"coffee maker\": \"cafetera\",\n",
    "    \"hair dryer\": \"secadora_cabello\",\n",
    "    \"iron\": \"plancha\",\n",
    "    \"iron board\": \"tabla_planchar\",\n",
    "    \"balcony\": \"balcon\",\n",
    "    \"terrace\": \"terraza\",\n",
    "    \"ocean view\": \"vista_mar\",\n",
    "    \"sea view\": \"vista_mar\",\n",
    "    \"garden view\": \"vista_jardin\",\n",
    "    \"city view\": \"vista_ciudad\",\n",
    "    \"soundproof\": \"insonorizado\",\n",
    "    \"elevator\": \"elevador\",\n",
    "    \"lift\": \"elevador\",\n",
    "    \"wheelchair\": \"silla_ruedas\",\n",
    "    \"wheelchair accessible\": \"acceso_silla_ruedas\",\n",
    "    \"roll-in shower\": \"ducha_accesible\",\n",
    "    \"accessible room\": \"habitacion_accesible\",\n",
    "\n",
    "    # --- Limpieza / plagas / olores ---\n",
    "    \"mold\": \"moho\",\n",
    "    \"mildew\": \"moho\",\n",
    "    \"leak\": \"fuga\",\n",
    "    \"leaking\": \"fuga\",\n",
    "    \"stain\": \"mancha\",\n",
    "    \"stains\": \"manchas\",\n",
    "    \"dusty\": \"polvoso\",\n",
    "    \"dirty\": \"sucio\",\n",
    "    \"smelly\": \"mal_olor\",\n",
    "    \"bad smell\": \"mal_olor\",\n",
    "    \"odor\": \"olor\",\n",
    "    \"bedbug\": \"chinche_cama\",\n",
    "    \"bedbugs\": \"chinches_cama\",\n",
    "    \"cockroach\": \"cucaracha\",\n",
    "    \"cockroaches\": \"cucarachas\",\n",
    "    \"ants\": \"hormigas\",\n",
    "    \"bugs\": \"insectos\",\n",
    "\n",
    "    # --- Personal / trato ---\n",
    "    \"staff\": \"personal\",\n",
    "    \"friendly\": \"amable\",\n",
    "    \"helpful\": \"servicial\",\n",
    "    \"rude\": \"grosero\",\n",
    "    \"impolite\": \"descortes\",\n",
    "    \"unprofessional\": \"poco_profesional\",\n",
    "    \"manager\": \"gerente\",\n",
    "    \"host\": \"anfitrion\",\n",
    "    \"hostess\": \"anfitriona\",\n",
    "    \"waiter\": \"mesero\",\n",
    "    \"waitress\": \"mesera\",\n",
    "    \"server\": \"mesero\",\n",
    "    \"bartender\": \"barman\",\n",
    "    \"lifeguard\": \"salvavidas\",\n",
    "\n",
    "    # --- Comida / bebida ---\n",
    "    \"breakfast\": \"desayuno\",\n",
    "    \"buffet\": \"buffet\",\n",
    "    \"brunch\": \"brunch\",\n",
    "    \"lunch\": \"comida\",\n",
    "    \"dinner\": \"cena\",\n",
    "    \"dessert\": \"postre\",\n",
    "    \"appetizer\": \"entrada\",\n",
    "    \"main course\": \"plato_fuerte\",\n",
    "    \"menu\": \"menu\",\n",
    "    \"dish\": \"platillo\",\n",
    "    \"portion\": \"porcion\",\n",
    "    \"taste\": \"sabor\",\n",
    "    \"tasteless\": \"insipido\",\n",
    "    \"delicious\": \"delicioso\",\n",
    "    \"undercooked\": \"crudo\",\n",
    "    \"overcooked\": \"sobre_cocido\",\n",
    "    \"cold food\": \"comida_fria\",\n",
    "    \"hot food\": \"comida_caliente\",\n",
    "    \"drink\": \"bebida\",\n",
    "    \"drinks\": \"bebidas\",\n",
    "    \"beverage\": \"bebida\",\n",
    "    \"beer\": \"cerveza\",\n",
    "    \"wine\": \"vino\",\n",
    "    \"cocktail\": \"coctel\",\n",
    "    \"water\": \"agua\",\n",
    "    \"ice\": \"hielo\",\n",
    "    \"inclusive drinks\": \"bebidas_incluidas\",\n",
    "\n",
    "    # --- Zonas / amenidades ---\n",
    "    \"pool\": \"alberca\",\n",
    "    \"infinity pool\": \"alberca_infinita\",\n",
    "    \"pool bar\": \"bar_alberca\",\n",
    "    \"beach\": \"playa\",\n",
    "    \"beach towel\": \"toalla_playa\",\n",
    "    \"private beach\": \"playa_privada\",\n",
    "    \"spa\": \"spa\",\n",
    "    \"sauna\": \"sauna\",\n",
    "    \"steam room\": \"vapor\",\n",
    "    \"gym\": \"gimnasio\",\n",
    "    \"kids club\": \"club_ninos\",\n",
    "    \"playground\": \"area_juegos\",\n",
    "    \"shuttle\": \"transporte\",\n",
    "    \"airport shuttle\": \"transporte_aeropuerto\",\n",
    "    \"uber\": \"uber\",\n",
    "    \"taxi\": \"taxi\",\n",
    "    \"tour\": \"tour\",\n",
    "    \"excursion\": \"excursion\",\n",
    "\n",
    "    # --- TecnologÃ­a / internet ---\n",
    "    \"wifi\": \"wifi\",\n",
    "    \"wi-fi\": \"wifi\",\n",
    "    \"internet\": \"internet\",\n",
    "    \"signal\": \"senal\",\n",
    "    \"coverage\": \"cobertura\",\n",
    "    \"password\": \"contrasena\",\n",
    "    \"tv\": \"tv\",\n",
    "    \"smart tv\": \"smart_tv\",\n",
    "    \"channel\": \"canal\",\n",
    "    \"channels\": \"canales\",\n",
    "    \"remote control\": \"control_remoto\",\n",
    "    \"outlet\": \"contacto\",\n",
    "    \"power outlet\": \"contacto\",\n",
    "    \"charger\": \"cargador\",\n",
    "\n",
    "    # --- Precios / pagos / cargos ---\n",
    "    \"price\": \"precio\",\n",
    "    \"expensive\": \"caro\",\n",
    "    \"cheap\": \"barato\",\n",
    "    \"fee\": \"cargo\",\n",
    "    \"charge\": \"cargo\",\n",
    "    \"extra charge\": \"cargo_extra\",\n",
    "    \"hidden fee\": \"cargo_oculto\",\n",
    "    \"tip\": \"propina\",\n",
    "    \"tips\": \"propinas\",\n",
    "\n",
    "    # --- Ruido / seguridad ---\n",
    "    \"noise\": \"ruido\",\n",
    "    \"noisy\": \"ruidoso\",\n",
    "    \"thin walls\": \"paredes_delgadas\",\n",
    "    \"security\": \"seguridad\",\n",
    "    \"safe\": \"seguro\",\n",
    "    \"unsafe\": \"inseguro\",\n",
    "    \"theft\": \"robo\",\n",
    "    \"stolen\": \"robado\",\n",
    "\n",
    "    # --- Valoraciones rÃ¡pidas ---\n",
    "    \"amazing\": \"increible\",\n",
    "    \"awesome\": \"increible\",\n",
    "    \"great\": \"genial\",\n",
    "    \"good\": \"bueno\",\n",
    "    \"ok\": \"regular\",\n",
    "    \"average\": \"promedio\",\n",
    "    \"bad\": \"malo\",\n",
    "    \"terrible\": \"terrible\",\n",
    "    \"awful\": \"horrible\",\n",
    "    \"horrible\": \"horrible\",\n",
    "    \"disappointing\": \"decepcionante\",\n",
    "    \"worth it\": \"vale_la_pena\",\n",
    "    \"not worth it\": \"no_vale_la_pena\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07a0e1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_es = set(SPACY_STOP_ES)\n",
    "negaciones = {\"no\",\"nunca\",\"jamÃ¡s\",\"jamas\",\"ni\",\"sin\", \"tampoco\",\"nadie\",\"nada\",\n",
    "              \"ningun\",\"ningÃºn\",\"ninguno\",\"ninguna\",\"ningunos\",\"ningunas\"}\n",
    "NEG_SINGLE = {\"no\",\"nunca\",\"jamas\",\"jamÃ¡s\",\"tampoco\",\"nadie\",\"nada\",\"ni\",\"sin\",\n",
    "              \"ningun\",\"ningÃºn\",\"ninguno\",\"ninguna\",\"ningunos\",\"ningunas\"}\n",
    "NEG_BIGRAMS = {(\"ya\",\"no\"),(\"ni\",\"siquiera\"),(\"para\",\"nada\"),(\"en\",\"absoluto\"),(\"sin\",\"que\")}\n",
    "stop_es_base = stop_es - negaciones\n",
    "stop_en_basic = {\"the\",\"a\",\"an\",\"and\",\"or\",\"but\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"to\",\"of\",\n",
    "                 \"in\",\"on\",\"at\",\"for\",\"with\",\"by\",\"about\",\"from\",\"it\",\"this\",\"that\",\"very\",\n",
    "                 \"really\",\"so\",\"just\",\"only\",\"my\",\"our\",\"your\",\"their\",\"we\",\"they\"}\n",
    "TIME_UNITS = {\"am\",\"pm\",\"hora\",\"horas\",\"minuto\",\"minutos\",\"segundo\",\"segundos\"}\n",
    "\n",
    "_re_url   = re.compile(r\"https?://\\S+|www\\.\\S+\", re.I)\n",
    "_re_email = re.compile(r\"\\b[\\w.+-]+@[\\w-]+\\.[\\w.-]+\\b\")\n",
    "_re_money = re.compile(r\"(?<!\\d)(\\$)\\s*\\d+[.,]?\\d*\")\n",
    "_re_num = re.compile(r\"(?<![\\w%$])\\d+[.,]?\\d*(?![\\w%$])\")\n",
    "_re_space = re.compile(r\"\\s+\")\n",
    "_re_risa  = re.compile(r\"j+a+(j+a+)+\", re.I)\n",
    "_re_repe  = re.compile(r\"(.)\\1{2,}\", re.UNICODE)\n",
    "_re_emoji = re.compile(\"[\" \"\\U0001F300-\\U0001FAFF\" \"\\U00002700-\\U000027BF\" \"\\U00002600-\\U000026FF\" \"]+\")\n",
    "_re_punct = re.compile(r\"[^\\w\\s%$]\")\n",
    "\n",
    "# Carga spaCy en espaÃ±ol, excluyendo lo que no se necesita\n",
    "nlp = spacy.load(\"es_core_news_sm\", exclude=[\"ner\",\"parser\",\"senter\",\"textcat\",\"tok2vec\"])\n",
    "BATCH, NPROC = 4000, 6\n",
    "\n",
    "# Emoji Lexicon\n",
    "emo_df  = pd.read_excel(\"Recursos Profe/Emojis lexicon.XLSX\")\n",
    "emo_df.columns = [str(c).strip().lower() for c in emo_df.columns]\n",
    "emo_cols = [\"anger\",\"anticipation\",\"disgust\",\"fear\",\"joy\",\"sadness\",\"surprise\",\"trust\"]\n",
    "sent_cols = [\"negative\",\"positive\"]\n",
    "emoji_list = emo_df[\"emoji\"].astype(str).tolist()\n",
    "weights_emotions = {\n",
    "    e: emo_df.set_index(\"emoji\")[e].to_dict() for e in emo_cols\n",
    "}\n",
    "weights_sent = {\n",
    "    s: emo_df.set_index(\"emoji\")[s].to_dict() for s in sent_cols\n",
    "}\n",
    "\n",
    "def emoji_weighted_features(raw_text: str):\n",
    "    # Cuenta ocurrencias por emoji (algunos pueden repetirse)\n",
    "    # Nota: .count() funciona bien para emojis de un caracter; si tuvieras secuencias, usa regex.\n",
    "    counts = {em: raw_text.count(em) for em in emoji_list}\n",
    "    total = sum(counts.values())\n",
    "\n",
    "    # Acumula puntajes ponderados por ocurrencia\n",
    "    feat = {}\n",
    "    # a) emociones bÃ¡sicas\n",
    "    for e in emo_cols:\n",
    "        s = 0.0\n",
    "        wdict = weights_emotions[e]\n",
    "        for em, c in counts.items():\n",
    "            if c:\n",
    "                w = wdict.get(em, 0.0)\n",
    "                s += c * float(w)\n",
    "        feat[f\"emoji_{e}_score\"] = s\n",
    "\n",
    "    # b) polaridad agregada\n",
    "    pos = neg = 0.0\n",
    "    for em, c in counts.items():\n",
    "        if c:\n",
    "            pos += c * float(weights_sent[\"positive\"].get(em, 0.0))\n",
    "            neg += c * float(weights_sent[\"negative\"].get(em, 0.0))\n",
    "    feat[\"emoji_pos_score\"] = pos\n",
    "    feat[\"emoji_neg_score\"] = neg\n",
    "    feat[\"emoji_score\"] = pos - neg\n",
    "\n",
    "    # c) conteos crudos y densidades\n",
    "    feat[\"emoji_count\"] = int(total)\n",
    "    # normaliza por longitud para que no â€œpremieâ€ textos largos (opcional)\n",
    "    L = max(len(raw_text), 1)\n",
    "    feat[\"emoji_density\"] = total / L\n",
    "\n",
    "    return pd.Series(feat)\n",
    "\n",
    "# LemtacizaciÃ³n\n",
    "def rejoin_negations_after_lemma(text_lemmatized: str):\n",
    "    toks = text_lemmatized.split()\n",
    "    out = []; i = 0\n",
    "    while i < len(toks):\n",
    "        tk = toks[i]\n",
    "        # Bigramas\n",
    "        if i+1 < len(toks) and (tk, toks[i+1]) in NEG_BIGRAMS:\n",
    "            pair = f\"{tk}_{toks[i+1]}\"\n",
    "            if i+2 < len(toks) and toks[i+2] not in stop_es_base:\n",
    "                out.append(f\"{pair}_{toks[i+2]}\"); i += 3\n",
    "            else:\n",
    "                out.append(pair); i += 2\n",
    "            continue\n",
    "        # 'sin' solo\n",
    "        if tk == \"sin\":\n",
    "            if i+1 < len(toks) and toks[i+1] not in stop_es_base:\n",
    "                out.append(f\"sin_{toks[i+1]}\"); i += 2\n",
    "            else:\n",
    "                out.append(\"sin\"); i += 1\n",
    "            continue\n",
    "        # Un solo token (no, nunca, ni, tampoco, ningÃºnâ€¦)\n",
    "        if tk in NEG_SINGLE:\n",
    "            if i+1 < len(toks):\n",
    "                nxt = toks[i+1]\n",
    "                if (nxt not in stop_es_base and nxt != \"num\" and nxt not in TIME_UNITS):\n",
    "                    out.append(f\"{tk}_{nxt}\"); i += 2; continue\n",
    "            out.append(tk); i += 1; continue\n",
    "        out.append(tk); i += 1\n",
    "    return \" \".join(out)\n",
    "\n",
    "# Funciones de normalizaciÃ³n\n",
    "def strip_accents(s): \n",
    "    nfkd = unicodedata.normalize(\"NFKD\", s)\n",
    "    return \"\".join(ch for ch in nfkd if not unicodedata.combining(ch))\n",
    "\n",
    "def reduce_repeated(text): return _re_repe.sub(r\"\\1\\1\", text)\n",
    "\n",
    "def replace_en_domain_words(text):\n",
    "    for en, es in en2es.items():\n",
    "        text = re.sub(rf\"\\b{re.escape(en)}\\b\", es, text, flags=re.I)\n",
    "    return text\n",
    "\n",
    "def normalize_text_for_lemma(s):\n",
    "    s = str(s).lower().replace(\"â€™\",\"'\").replace(\"â€œ\",'\"').replace(\"â€\",'\"')\n",
    "    s = _re_url.sub(\" url \", s); s = _re_email.sub(\" email \", s)\n",
    "    s = s.replace(\":)\", \" emoticon_positivo \").replace(\"(:\", \" emoticon_positivo \")\n",
    "    s = s.replace(\":(\", \" emoticon_negativo \").replace(\"):\", \" emoticon_negativo \")\n",
    "    s = _re_emoji.sub(\" emoji \", s)\n",
    "    s = _re_risa.sub(\" jaja \", s); s = reduce_repeated(s)\n",
    "    s = _re_money.sub(\" dinero \", s); s = _re_num.sub(\" num \", s)\n",
    "    \n",
    "    s = _re_punct.sub(\" \", s)\n",
    "    s = replace_en_domain_words(s)\n",
    "    s = _re_space.sub(\" \", s).strip()\n",
    "    toks = [t for t in s.split() if (t not in stop_es_base and t not in stop_en_basic)]\n",
    "    return \" \".join(toks)\n",
    "\n",
    "# SEL\n",
    "sel = pd.read_csv(\"Recursos Profe/SEL_full.txt\", sep=r\"\\s*\\t\\s*\", engine=\"python\")\n",
    "sel[\"key\"] = sel[\"Palabra\"].str.lower().apply(strip_accents)\n",
    "pfa = dict(zip(sel[\"key\"], sel[\"PFA\"]))\n",
    "cat = dict(zip(sel[\"key\"], sel[\"CategorÃ­a\"].str.lower()))\n",
    "emociones = sorted(sel[\"CategorÃ­a\"].str.lower().unique())\n",
    "\n",
    "def sel_features(text_norm):\n",
    "    toks = text_norm.split()\n",
    "    score = 0.0\n",
    "    counts = {e: 0 for e in emociones}\n",
    "    for t in toks:\n",
    "        w = t.replace(\"no_\",\"\"); inv = t.startswith(\"no_\")\n",
    "        if w in pfa: score += (-pfa[w] if inv else pfa[w])\n",
    "        if w in cat: counts[cat[w]] += (-1 if inv else 1)\n",
    "    counts[\"sel_score\"] = score\n",
    "    return pd.Series(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493dbd9a",
   "metadata": {},
   "source": [
    "Aplicar al dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f33d643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza rÃ¡pida\n",
    "df[\"text_clean\"] = df[\"text\"].apply(normalize_text_for_lemma)\n",
    "texts = df[\"text_clean\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81109d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LematizaciÃ³n en lote con spaCy\n",
    "lemmas = []\n",
    "for doc in nlp.pipe(texts, batch_size=BATCH, n_process=NPROC):\n",
    "    lemmas.append(\" \".join(t.lemma_ for t in doc))\n",
    "\n",
    "lemmas = [strip_accents(t) for t in lemmas]\n",
    "\n",
    "df[\"text_norm\"] = [rejoin_negations_after_lemma(t) for t in lemmas]\n",
    "df.drop(columns=[\"text_clean\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f50bc139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emoji lexicon\n",
    "df[[\n",
    "    \"emoji_anger_score\",\"emoji_anticipation_score\",\"emoji_disgust_score\",\"emoji_fear_score\",\n",
    "    \"emoji_joy_score\",\"emoji_sadness_score\",\"emoji_surprise_score\",\"emoji_trust_score\",\n",
    "    \"emoji_pos_score\",\"emoji_neg_score\",\"emoji_score\",\"emoji_count\",\"emoji_density\"\n",
    "]] = df[\"text\"].apply(emoji_weighted_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e6005f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEL\n",
    "df[[\"sel_score\", *emociones]] = df[\"text_norm\"].apply(sel_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "358ac0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Opinion</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Attraction</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_norm</th>\n",
       "      <th>emoji_anger_score</th>\n",
       "      <th>emoji_anticipation_score</th>\n",
       "      <th>emoji_disgust_score</th>\n",
       "      <th>...</th>\n",
       "      <th>emoji_score</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>emoji_density</th>\n",
       "      <th>sel_score</th>\n",
       "      <th>alegrÃ­a</th>\n",
       "      <th>enojo</th>\n",
       "      <th>miedo</th>\n",
       "      <th>repulsiÃ³n</th>\n",
       "      <th>sorpresa</th>\n",
       "      <th>tristeza</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PÃ©simo lugar</td>\n",
       "      <td>Piensen dos veces antes de ir a este hotel, te...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel</td>\n",
       "      <td>PÃ©simo lugar Piensen dos veces antes de ir a e...</td>\n",
       "      <td>1</td>\n",
       "      <td>pesimo lugar piensen hotel molestan hijo anos ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No vayas a lugar de Eddie</td>\n",
       "      <td>Cuatro de nosotros fuimos recientemente a Eddi...</td>\n",
       "      <td>1</td>\n",
       "      <td>Restaurant</td>\n",
       "      <td>No vayas a lugar de Eddie Cuatro de nosotros f...</td>\n",
       "      <td>1</td>\n",
       "      <td>no_vayas lugar eddie recientemente eddie s pla...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mala relaciÃ³n calidad-precio</td>\n",
       "      <td>seguirÃ© corta y simple: limpieza\\n- bad. Tengo...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel</td>\n",
       "      <td>Mala relaciÃ³n calidad-precio seguirÃ© corta y s...</td>\n",
       "      <td>1</td>\n",
       "      <td>mala relacion calidad precio seguire corta sim...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MinusvÃ¡lido? Â¡No te alojes aquÃ­!</td>\n",
       "      <td>Al reservar un hotel con multipropiedad Mayan ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel</td>\n",
       "      <td>MinusvÃ¡lido? Â¡No te alojes aquÃ­! Al reservar u...</td>\n",
       "      <td>1</td>\n",
       "      <td>minusvalido no_alojes reservar hotel multiprop...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Es una porqueria no pierdan su tiempo</td>\n",
       "      <td>No pierdan su tiempo ni dinero, venimos porque...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel</td>\n",
       "      <td>Es una porqueria no pierdan su tiempo No pierd...</td>\n",
       "      <td>1</td>\n",
       "      <td>porqueria no_pierdan tiempo no_pierdan tiempo ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Title  \\\n",
       "0                           PÃ©simo lugar   \n",
       "1              No vayas a lugar de Eddie   \n",
       "2           Mala relaciÃ³n calidad-precio   \n",
       "3       MinusvÃ¡lido? Â¡No te alojes aquÃ­!   \n",
       "4  Es una porqueria no pierdan su tiempo   \n",
       "\n",
       "                                             Opinion  Polarity  Attraction  \\\n",
       "0  Piensen dos veces antes de ir a este hotel, te...         1       Hotel   \n",
       "1  Cuatro de nosotros fuimos recientemente a Eddi...         1  Restaurant   \n",
       "2  seguirÃ© corta y simple: limpieza\\n- bad. Tengo...         1       Hotel   \n",
       "3  Al reservar un hotel con multipropiedad Mayan ...         1       Hotel   \n",
       "4  No pierdan su tiempo ni dinero, venimos porque...         1       Hotel   \n",
       "\n",
       "                                                text  target  \\\n",
       "0  PÃ©simo lugar Piensen dos veces antes de ir a e...       1   \n",
       "1  No vayas a lugar de Eddie Cuatro de nosotros f...       1   \n",
       "2  Mala relaciÃ³n calidad-precio seguirÃ© corta y s...       1   \n",
       "3  MinusvÃ¡lido? Â¡No te alojes aquÃ­! Al reservar u...       1   \n",
       "4  Es una porqueria no pierdan su tiempo No pierd...       1   \n",
       "\n",
       "                                           text_norm  emoji_anger_score  \\\n",
       "0  pesimo lugar piensen hotel molestan hijo anos ...                0.0   \n",
       "1  no_vayas lugar eddie recientemente eddie s pla...                0.0   \n",
       "2  mala relacion calidad precio seguire corta sim...                0.0   \n",
       "3  minusvalido no_alojes reservar hotel multiprop...                0.0   \n",
       "4  porqueria no_pierdan tiempo no_pierdan tiempo ...                0.0   \n",
       "\n",
       "   emoji_anticipation_score  emoji_disgust_score  ...  emoji_score  \\\n",
       "0                       0.0                  0.0  ...          0.0   \n",
       "1                       0.0                  0.0  ...          0.0   \n",
       "2                       0.0                  0.0  ...          0.0   \n",
       "3                       0.0                  0.0  ...          0.0   \n",
       "4                       0.0                  0.0  ...          0.0   \n",
       "\n",
       "   emoji_count  emoji_density  sel_score  alegrÃ­a  enojo  miedo  repulsiÃ³n  \\\n",
       "0          0.0            0.0        0.0      0.0    0.0    1.0        0.0   \n",
       "1          0.0            0.0        0.0      0.0    1.0    0.0        0.0   \n",
       "2          0.0            0.0        0.0      0.0    0.0    0.0        0.0   \n",
       "3          0.0            0.0        1.0      0.0    0.0    0.0        1.0   \n",
       "4          0.0            0.0        0.0      0.0    0.0    0.0        1.0   \n",
       "\n",
       "   sorpresa  tristeza  \n",
       "0       0.0     0.630  \n",
       "1       0.0     0.898  \n",
       "2       1.0     0.265  \n",
       "3       0.0     1.529  \n",
       "4       1.0     0.794  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab645233",
   "metadata": {},
   "source": [
    "Se exporta archivo normalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7680c43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"Rest_Mex_2022_preprocessed.xlsx\", index=False, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c884353c",
   "metadata": {},
   "source": [
    "# **Parte 2 Alberto**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a31787",
   "metadata": {},
   "source": [
    "**Configurar columnas y asegurar split 80/20**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2eb7210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack, csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ec14409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Title', 'Opinion', 'Polarity', 'Attraction', 'text', 'target',\n",
      "       'text_norm', 'emoji_anger_score', 'emoji_anticipation_score',\n",
      "       'emoji_disgust_score', 'emoji_fear_score', 'emoji_joy_score',\n",
      "       'emoji_sadness_score', 'emoji_surprise_score', 'emoji_trust_score',\n",
      "       'emoji_pos_score', 'emoji_neg_score', 'emoji_score', 'emoji_count',\n",
      "       'emoji_density', 'sel_score', 'alegrÃ­a', 'enojo', 'miedo', 'repulsiÃ³n',\n",
      "       'sorpresa', 'tristeza'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PÃ©simo lugar Piensen dos veces antes de ir a e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No vayas a lugar de Eddie Cuatro de nosotros f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mala relaciÃ³n calidad-precio seguirÃ© corta y s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MinusvÃ¡lido? Â¡No te alojes aquÃ­! Al reservar u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Es una porqueria no pierdan su tiempo No pierd...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  PÃ©simo lugar Piensen dos veces antes de ir a e...       1\n",
       "1  No vayas a lugar de Eddie Cuatro de nosotros f...       1\n",
       "2  Mala relaciÃ³n calidad-precio seguirÃ© corta y s...       1\n",
       "3  MinusvÃ¡lido? Â¡No te alojes aquÃ­! Al reservar u...       1\n",
       "4  Es una porqueria no pierdan su tiempo No pierd...       1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df.columns)\n",
    "\n",
    "# Ajustar nombres segÃºn lo que ya creaste\n",
    "TEXT_COL = 'text'      # columna con Title + Opinion concatenado\n",
    "LABEL_COL = 'target'   # o 'Polarity' \n",
    "\n",
    "display(df[[TEXT_COL, LABEL_COL]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3badfaf",
   "metadata": {},
   "source": [
    "**Split 80/20 con shuffle=True y random_state=0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "506f7a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TamaÃ±o train: 24169\n",
      "TamaÃ±o test: 6043\n"
     ]
    }
   ],
   "source": [
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    df[TEXT_COL],\n",
    "    df[LABEL_COL],\n",
    "    test_size=0.2,\n",
    "    shuffle=True,\n",
    "    random_state=0,\n",
    "    stratify=df[LABEL_COL]  #por el desbalance\n",
    ")\n",
    "\n",
    "print(\"TamaÃ±o train:\", len(X_train_text))\n",
    "print(\"TamaÃ±o test:\", len(X_test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1048ed31",
   "metadata": {},
   "source": [
    "**Funciones de vectorizaciÃ³n (binaria, frecuencia, TF-IDF)**  \n",
    "Estas funciones:\n",
    "\n",
    "Transforman texto â†’ matriz numÃ©rica.\n",
    "\n",
    "Mantienen el mismo split porque el vectorizador se ajusta SOLO con X_train_text y luego se aplica a X_test_text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3d89506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_binary(X_train_text, X_test_text, **kwargs):\n",
    "    \"\"\"\n",
    "    RepresentaciÃ³n binarizada: 1 si la palabra aparece, 0 si no.\n",
    "    \"\"\"\n",
    "    vect_bin = CountVectorizer(binary=True, **kwargs)\n",
    "    X_train_bin = vect_bin.fit_transform(X_train_text)\n",
    "    X_test_bin = vect_bin.transform(X_test_text)\n",
    "    return X_train_bin, X_test_bin, vect_bin\n",
    "\n",
    "def vectorize_frequency(X_train_text, X_test_text, **kwargs):\n",
    "    \"\"\"\n",
    "    RepresentaciÃ³n por frecuencia: conteo de ocurrencias de cada tÃ©rmino.\n",
    "    \"\"\"\n",
    "    vect_freq = CountVectorizer(binary=False, **kwargs)\n",
    "    X_train_freq = vect_freq.fit_transform(X_train_text)\n",
    "    X_test_freq = vect_freq.transform(X_test_text)\n",
    "    return X_train_freq, X_test_freq, vect_freq\n",
    "\n",
    "def vectorize_tfidf(X_train_text, X_test_text, **kwargs):\n",
    "    \"\"\"\n",
    "    RepresentaciÃ³n TF-IDF: pondera tÃ©rminos segÃºn importancia en documentos.\n",
    "    \"\"\"\n",
    "    vect_tfidf = TfidfVectorizer(**kwargs)\n",
    "    X_train_tfidf = vect_tfidf.fit_transform(X_train_text)\n",
    "    X_test_tfidf = vect_tfidf.transform(X_test_text)\n",
    "    return X_train_tfidf, X_test_tfidf, vect_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c7de5d",
   "metadata": {},
   "source": [
    "**Ejemplo rÃ¡pido para generar las tres:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d9d1121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binaria: (24169, 45691) (6043, 45691)\n",
      "Frecuencia: (24169, 45691) (6043, 45691)\n",
      "TF-IDF: (24169, 45691) (6043, 45691)\n"
     ]
    }
   ],
   "source": [
    "X_train_bin, X_test_bin, vect_bin = vectorize_binary(X_train_text, X_test_text)\n",
    "X_train_freq, X_test_freq, vect_freq = vectorize_frequency(X_train_text, X_test_text)\n",
    "X_train_tfidf, X_test_tfidf, vect_tfidf = vectorize_tfidf(X_train_text, X_test_text)\n",
    "\n",
    "print(\"Binaria:\", X_train_bin.shape, X_test_bin.shape)\n",
    "print(\"Frecuencia:\", X_train_freq.shape, X_test_freq.shape)\n",
    "print(\"TF-IDF:\", X_train_tfidf.shape, X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99268d49",
   "metadata": {},
   "source": [
    "**LÃ©xicos de sentimiento (features extra)**  \n",
    "AquÃ­ agregamos conteo de palabras positivas y negativas por documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7165340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo simple de lÃ©xicos (AMPLÃA ESTO para la prÃ¡ctica real)\n",
    "positive_lexicon = {\n",
    "    \"bueno\",\"excelente\",\"perfecto\",\"maravilloso\",\"increÃ­ble\",\"bonito\",\"agradable\",\n",
    "    \"recomendado\",\"limpio\",\"rÃ¡pido\",\"delicioso\",\"rico\",\"amable\",\"fantÃ¡stico\",\"genial\"\n",
    "}\n",
    "\n",
    "negative_lexicon = {\n",
    "    \"malo\",\"terrible\",\"horrible\",\"pÃ©simo\",\"lento\",\"sucio\",\"caro\",\n",
    "    \"decepcionante\",\"desagradable\",\"asqueroso\",\"frÃ­o\",\"tardado\",\"estresante\",\"feo\"\n",
    "}\n",
    "\n",
    "def count_sentiment_words(text, pos_lex=positive_lexicon, neg_lex=negative_lexicon):\n",
    "    tokens = text.split()\n",
    "    pos_count = sum(1 for t in tokens if t in pos_lex)\n",
    "    neg_count = sum(1 for t in tokens if t in neg_lex)\n",
    "    return pos_count, neg_count\n",
    "\n",
    "def build_lexicon_features(text_series):\n",
    "    \"\"\"\n",
    "    Regresa una matriz numpy de shape (n_docs, 2):\n",
    "    [conteo_pos, conteo_neg]\n",
    "    \"\"\"\n",
    "    feats = [count_sentiment_words(str(t)) for t in text_series]\n",
    "    return np.array(feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70d3ba0",
   "metadata": {},
   "source": [
    "**prueba**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8eec752b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LÃ©xicos train: (24169, 2)\n",
      "LÃ©xicos test: (6043, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 0],\n",
       "       [1, 0],\n",
       "       [0, 0],\n",
       "       [5, 1]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lex_train = build_lexicon_features(X_train_text)\n",
    "lex_test = build_lexicon_features(X_test_text)\n",
    "\n",
    "print(\"LÃ©xicos train:\", lex_train.shape)\n",
    "print(\"LÃ©xicos test:\", lex_test.shape)\n",
    "lex_train[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a059187",
   "metadata": {},
   "source": [
    "**Emojis / Emoticonos (features extra)**  \n",
    "Features tÃ­picos:\n",
    "\n",
    "Conteo de emojis/emoticonos positivos.\n",
    "\n",
    "Conteo de emojis/emoticonos negativos.\n",
    "\n",
    "(Opcional) Conteo de signos de exclamaciÃ³n para intensidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04cd9db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emojis positivos detectados: {'ğŸ¸', 'ğŸ’¤', 'ğŸŒ¹', 'ğŸŠ', 'ğŸ™Œ', 'ğŸ‘¦', 'ğŸ˜œ', 'ğŸ™', 'ğŸ¤©', 'ğŸ˜¬', 'ğŸ¤˜', 'ğŸ¼', 'ğŸª‚', 'ğŸ²', 'â™¡', 'ğŸ˜€', 'â˜º', 'ğŸ„', 'ğŸ˜', 'ğŸ­', 'ğŸ¦ª', 'ğŸ‰', 'ğŸ¥‚', 'âœŒ', 'ğŸŒ', 'ğŸ’§', 'ğŸ¤', 'ğŸ‘§', 'â™€', 'ğŸ¿', 'ğŸ»', 'ğŸº', 'ğŸ¤ª', 'ğŸ¾', 'ğŸ‚', 'ğŸ»', 'ğŸŒ­', 'â˜€', 'ğŸ”Ÿ', 'ğŸ‘', 'ğŸ˜Š', 'ğŸ‡', 'ğŸ¥—', 'ğŸ‘©', 'ğŸ¸', 'ğŸ¤£', 'ğŸ˜„', 'ğŸ‘¯', 'ğŸŒ´', 'ğŸ¥°', 'ğŸ¶', 'ğŸ˜ƒ', 'ğŸ¥¤', 'ğŸ‘Œ', 'ğŸ¥', 'ğŸ˜š', 'ğŸ¦', 'ğŸ‘', 'ğŸ”¥', 'ğŸ¤™', 'ğŸŒ»', 'ğŸ˜Œ', 'ğŸ–', 'ğŸˆ', 'ğŸ¥º', 'âœ”', 'ğŸ™‚', 'ğŸ‘', 'âœ‹', 'ğŸŒ…', 'â˜¹', 'ğŸŸ', 'ğŸ­', 'ğŸ‘Š', 'ğŸ¦', 'ğŸŒ®', 'ğŸ˜†', 'ğŸ–¤', 'ğŸŒˆ', 'ğŸ²', 'ğŸ¤¤', 'ğŸ¡', 'ğŸ’†', 'â™§', 'â™‚', 'ğŸ’™', 'âœ…', 'ğŸ©', 'ğŸ™', 'ğŸ‘ª', 'ğŸ’“', 'ğŸ˜‰', 'âœ¨', 'ğŸ…', 'ğŸ‘“', 'ğŸ”™', 'ğŸ¤½', 'ğŸ”', 'ğŸ·', 'ğŸ¤', 'ğŸ˜‹', 'ğŸ˜¸', 'ğŸŒ', 'â™¥', 'ğŸ˜ª', 'ğŸ¯', 'ğŸ˜‡', 'ğŸ˜ˆ', 'ğŸ’', 'â¤', 'ğŸ™ˆ', 'ğŸ•¶', 'ğŸ’', 'ğŸŠ', 'ğŸ˜­', 'ğŸ•', 'ğŸ¹', 'ğŸ¼', 'â›±', 'ğŸ™‹', 'ğŸ’–', 'ğŸ½', 'ğŸ‘…', 'ğŸ“', 'ğŸŒŠ', 'ğŸ‘™', 'ğŸ˜‘', 'ğŸ’', 'ğŸ˜‚', 'ğŸ’•', 'ğŸ˜', 'ğŸ', 'ğŸŒ·', 'â„', 'ğŸŒ‹', 'âš½', 'â—', 'ğŸ', 'ğŸŒš', 'ğŸ™Š', 'ğŸ’›', 'ğŸŒº', 'ğŸ‘€', 'ğŸ’ª', 'ğŸ‘¨', 'ğŸ’—', 'ğŸ˜', 'ğŸ£', 'ğŸ¾', 'ğŸ˜·', 'ğŸ˜–', 'ğŸ¤“', 'ğŸ˜', 'ğŸ™ƒ', 'ğŸ†', 'ğŸ”†', 'ğŸ¤Ÿ', 'ğŸ˜…', 'ğŸ¤ ', 'ğŸ’¯', 'ğŸ’œ', 'ğŸ¤—', 'ğŸ‘', 'ğŸ˜¢', 'ğŸ¦©', 'ğŸ¥˜', 'ğŸ¤”', 'ğŸ˜›', 'ğŸ¦', 'ğŸ', 'ğŸœ', 'ğŸ˜˜', 'ğŸ˜±', 'ğŸ˜¦', 'ğŸ‘‹', 'ğŸ¥³'}\n",
      "Emojis negativos detectados: {'ğŸ˜¡', 'ğŸ™', 'ğŸ˜ ', 'ğŸ˜”', 'ğŸ¤š', 'ğŸ˜“', 'ğŸ˜³'}\n",
      "Emoji feats train: (24169, 3)\n",
      "Emoji feats test: (6043, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_pattern = re.compile(\n",
    "    r'['\n",
    "    r'\\U0001F300-\\U0001F5FF'  # sÃ­mbolos y pictogramas\n",
    "    r'\\U0001F600-\\U0001F64F'  # emoticonos\n",
    "    r'\\U0001F680-\\U0001F6FF'  # transporte y mapas\n",
    "    r'\\U0001F700-\\U0001F77F'\n",
    "    r'\\U0001F780-\\U0001F7FF'\n",
    "    r'\\U0001F800-\\U0001F8FF'\n",
    "    r'\\U0001F900-\\U0001F9FF'\n",
    "    r'\\U0001FA00-\\U0001FAFF'\n",
    "    r'\\u2600-\\u26FF'          # sÃ­mbolos varios\n",
    "    r'\\u2700-\\u27BF'\n",
    "    r']'\n",
    ")\n",
    "\n",
    "def extract_emojis(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    return emoji_pattern.findall(text)\n",
    "\n",
    "# Definir positivo / negativo \n",
    "# 1 = muy negativo, 2 = negativo, 3 = neutro, 4 = positivo, 5 = muy positivo\n",
    "POS_LABELS = {4, 5}\n",
    "NEG_LABELS = {1, 2}\n",
    "\n",
    "pos_counts = {}\n",
    "neg_counts = {}\n",
    "\n",
    "for txt, label in zip(df[TEXT_COL], df[LABEL_COL]):\n",
    "    emojis = extract_emojis(txt)\n",
    "    if not emojis:\n",
    "        continue\n",
    "\n",
    "    # normalizamos label como string\n",
    "    lab = str(label).strip()\n",
    "\n",
    "    # conjuntos ya como strings para comparar\n",
    "    pos_set = {str(l) for l in POS_LABELS}\n",
    "    neg_set = {str(l) for l in NEG_LABELS}\n",
    "\n",
    "    if lab in pos_set:\n",
    "        for e in emojis:\n",
    "            pos_counts[e] = pos_counts.get(e, 0) + 1\n",
    "    elif lab in neg_set:\n",
    "        for e in emojis:\n",
    "            neg_counts[e] = neg_counts.get(e, 0) + 1\n",
    "\n",
    "# Emojis clasificados segÃºn en quÃ© clase aparecen mÃ¡s\n",
    "positive_emojis = {e for e in pos_counts if pos_counts[e] > neg_counts.get(e, 0)}\n",
    "negative_emojis = {e for e in neg_counts if neg_counts[e] > pos_counts.get(e, 0)}\n",
    "\n",
    "print(\"Emojis positivos detectados:\", positive_emojis)\n",
    "print(\"Emojis negativos detectados:\", negative_emojis)\n",
    "\n",
    "# Emoticonos comunes \n",
    "positive_emoticons = {\":)\", \":-)\", \":D\", \"(:\", \"=)\", \";)\", \";-)\", \":')\"}\n",
    "negative_emoticons = {\":(\", \":-(\", \"):\", \"='(\", \":'(\", \"D:\", \">:(\", \":-/\"}\n",
    "\n",
    "# Funciones de features\n",
    "def emoji_emoticon_features(text):\n",
    "    t = str(text)\n",
    "\n",
    "    # conteo de emojis segÃºn los sets aprendidos\n",
    "    pos = sum(ch in positive_emojis for ch in t)\n",
    "    neg = sum(ch in negative_emojis for ch in t)\n",
    "\n",
    "    # conteo de emoticonos\n",
    "    for emo in positive_emoticons:\n",
    "        pos += t.count(emo)\n",
    "    for emo in negative_emoticons:\n",
    "        neg += t.count(emo)\n",
    "\n",
    "    # intensidad con signos de exclamaciÃ³n\n",
    "    exclam = t.count(\"!\")\n",
    "\n",
    "    return pos, neg, exclam\n",
    "\n",
    "def build_emoji_features(text_series):\n",
    "    \"\"\"\n",
    "    Regresa matriz (n_docs, 3):\n",
    "    [emoji_pos+emo_pos, emoji_neg+emo_neg, exclamaciones]\n",
    "    \"\"\"\n",
    "    feats = [emoji_emoticon_features(t) for t in text_series]\n",
    "    return np.array(feats)\n",
    "\n",
    "# Generar features para train/test\n",
    "emoji_train = build_emoji_features(X_train_text)\n",
    "emoji_test = build_emoji_features(X_test_text)\n",
    "\n",
    "print(\"Emoji feats train:\", emoji_train.shape)\n",
    "print(\"Emoji feats test:\", emoji_test.shape)\n",
    "emoji_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d426ab",
   "metadata": {},
   "source": [
    "**Combinar representaciones + features extra**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "156e5c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF simple: (24169, 45691)\n",
      "TF-IDF + extras: (24169, 45696)\n"
     ]
    }
   ],
   "source": [
    "def add_extra_features(X_train_base, X_test_base, extra_train, extra_test):\n",
    "    \"\"\"\n",
    "    Concatena la matriz dispersa base con features densos extra.\n",
    "    \"\"\"\n",
    "    extra_train_sparse = csr_matrix(extra_train)\n",
    "    extra_test_sparse = csr_matrix(extra_test)\n",
    "    \n",
    "    X_train_comb = hstack([X_train_base, extra_train_sparse])\n",
    "    X_test_comb = hstack([X_test_base, extra_test_sparse])\n",
    "    \n",
    "    return X_train_comb, X_test_comb\n",
    "\n",
    "# Ejemplo: TF-IDF + lÃ©xicos + emojis\n",
    "extra_train = np.hstack([lex_train, emoji_train])\n",
    "extra_test = np.hstack([lex_test, emoji_test])\n",
    "\n",
    "X_train_tfidf_ext, X_test_tfidf_ext = add_extra_features(\n",
    "    X_train_tfidf, X_test_tfidf,\n",
    "    extra_train, extra_test\n",
    ")\n",
    "\n",
    "print(\"TF-IDF simple:\", X_train_tfidf.shape)\n",
    "print(\"TF-IDF + extras:\", X_train_tfidf_ext.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e73051",
   "metadata": {},
   "source": [
    "# **Tabla/resumen**  \n",
    "| RepresentaciÃ³n      | DescripciÃ³n                                                                 | Ventajas                                                   | Desventajas                                                    |\n",
    "|---------------------|-----------------------------------------------------------------------------|------------------------------------------------------------|----------------------------------------------------------------|\n",
    "| Binarizada          | Cada tÃ©rmino se representa con 0/1 segÃºn aparezca en el documento.         | Simple, reduce impacto de repeticiones, Ãºtil con muchos docs. | Pierde informaciÃ³n de frecuencia; menos expresiva.            |\n",
    "| Frecuencia (BoW)    | Cada tÃ©rmino es el nÃºmero de veces que aparece en el documento.            | FÃ¡cil de interpretar, buena base para muchos modelos.      | Favorece palabras muy frecuentes; no considera importancia global. |\n",
    "| TF-IDF              | Pondera frecuencia local vs. frecuencia global del tÃ©rmino.                | Resalta tÃ©rminos relevantes; muy usada en IR y NLP clÃ¡sico.| MÃ¡s compleja; puede sobreajustar si no se regula.              |\n",
    "| LÃ©xicos de sentimiento (extra) | Features numÃ©ricos con conteo de palabras positivas/negativas.          | Introduce conocimiento lingÃ¼Ã­stico explÃ­cito.             | Depende de la calidad/cobertura del lÃ©xico; sensible al dominio. |\n",
    "| Emojis/Emoticonos (extra)      | Conteo de emojis/emoticonos positivos/negativos e intensidad (!).       | Captura seÃ±ales afectivas tÃ­picas de redes/comentarios.   | No siempre aparecen; puede ser ruidoso si se usa solo.         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6dee45",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tf310)",
   "language": "python",
   "name": "tf310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
