{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0db3ca84",
   "metadata": {},
   "source": [
    "# Practice IV - Sentiment Analysis\n",
    "## **1ra Parte Corpus preprocessing David**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f176ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata, pandas as pd, os, spacy\n",
    "from spacy.lang.es.stop_words import STOP_WORDS as SPACY_STOP_ES\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28945a9d",
   "metadata": {},
   "source": [
    "Cargar Rest_Mex_2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3988e920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Opinion</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Attraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P√©simo lugar</td>\n",
       "      <td>Piensen dos veces antes de ir a este hotel, te...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No vayas a lugar de Eddie</td>\n",
       "      <td>Cuatro de nosotros fuimos recientemente a Eddi...</td>\n",
       "      <td>1</td>\n",
       "      <td>Restaurant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mala relaci√≥n calidad-precio</td>\n",
       "      <td>seguir√© corta y simple: limpieza\\n- bad. Tengo...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Minusv√°lido? ¬°No te alojes aqu√≠!</td>\n",
       "      <td>Al reservar un hotel con multipropiedad Mayan ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Es una porqueria no pierdan su tiempo</td>\n",
       "      <td>No pierdan su tiempo ni dinero, venimos porque...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>El peor huevos Benedict jam√°s</td>\n",
       "      <td>Hoy ten√≠amos el desayuno por segunda vez en po...</td>\n",
       "      <td>1</td>\n",
       "      <td>Restaurant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Definitivamente no volver√≠a a hospedarme en Sa...</td>\n",
       "      <td>El hotel en si no es malo, pero mi experiencia...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Terrible</td>\n",
       "      <td>No estoy seguro de por qu√© este restaurante ti...</td>\n",
       "      <td>1</td>\n",
       "      <td>Restaurant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bebidas ADULTERADAS, FATAL mi experiencia!!!</td>\n",
       "      <td>Llegu√© a este hotel ‚Äúpor desgracia‚Äù mi reserva...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hotel ha ido cuesta abajo</td>\n",
       "      <td>Hemos estado viniendo a Villa la Estancia dura...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0                                       P√©simo lugar   \n",
       "1                          No vayas a lugar de Eddie   \n",
       "2                       Mala relaci√≥n calidad-precio   \n",
       "3                   Minusv√°lido? ¬°No te alojes aqu√≠!   \n",
       "4              Es una porqueria no pierdan su tiempo   \n",
       "5                      El peor huevos Benedict jam√°s   \n",
       "6  Definitivamente no volver√≠a a hospedarme en Sa...   \n",
       "7                                           Terrible   \n",
       "8       Bebidas ADULTERADAS, FATAL mi experiencia!!!   \n",
       "9                          Hotel ha ido cuesta abajo   \n",
       "\n",
       "                                             Opinion  Polarity  Attraction  \n",
       "0  Piensen dos veces antes de ir a este hotel, te...         1       Hotel  \n",
       "1  Cuatro de nosotros fuimos recientemente a Eddi...         1  Restaurant  \n",
       "2  seguir√© corta y simple: limpieza\\n- bad. Tengo...         1       Hotel  \n",
       "3  Al reservar un hotel con multipropiedad Mayan ...         1       Hotel  \n",
       "4  No pierdan su tiempo ni dinero, venimos porque...         1       Hotel  \n",
       "5  Hoy ten√≠amos el desayuno por segunda vez en po...         1  Restaurant  \n",
       "6  El hotel en si no es malo, pero mi experiencia...         1       Hotel  \n",
       "7  No estoy seguro de por qu√© este restaurante ti...         1  Restaurant  \n",
       "8  Llegu√© a este hotel ‚Äúpor desgracia‚Äù mi reserva...         1       Hotel  \n",
       "9  Hemos estado viniendo a Villa la Estancia dura...         1       Hotel  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar corpus .xlsx\n",
    "df = pd.read_excel('Recursos Profe/Rest_Mex_2022.xlsx', engine='openpyxl')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "223e919b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30212 entries, 0 to 30211\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Title       30210 non-null  object\n",
      " 1   Opinion     30210 non-null  object\n",
      " 2   Polarity    30212 non-null  int64 \n",
      " 3   Attraction  30212 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 944.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58b0522a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30212, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7d6a99",
   "metadata": {},
   "source": [
    "Concatenar Title + Opinion como texto de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a29eb371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir las columnas 'Title' y 'Opinion' a tipo string.\n",
    "df['Title'] = df['Title'].astype(str)\n",
    "df['Opinion'] = df['Opinion'].astype(str)\n",
    "\n",
    "# Concatenar Title + Opinion como texto de entrada.\n",
    "df['text'] = df['Title'] + ' ' + df['Opinion']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1fb066",
   "metadata": {},
   "source": [
    "Dejar Polarity como etiqueta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd60d070",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = df['Polarity']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4a8eb1",
   "metadata": {},
   "source": [
    "Normalizaci√≥n base:\n",
    "- Min√∫sculas, quitar signos raros, espacios extra, etc.\n",
    "- Manejo b√°sico de stopwords.\n",
    "- Manejo de negaciones.\n",
    "- Manejo de repeticiones.\n",
    "- Correcci√≥n b√°sica de errores comunes.\n",
    "\n",
    "Se a√±ade:\n",
    "- Emojis lexicon\n",
    "- SEL_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83620cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear diccionario de traducci√≥n ingl√©s -> espa√±ol para t√©rminos comunes\n",
    "en2es = {\n",
    "    \"front desk\": \"recepcion\",\n",
    "    \"check-in\": \"check_in\",\n",
    "    \"check in\": \"check_in\",\n",
    "    \"early check-in\": \"check_in_temprano\",\n",
    "    \"late check-out\": \"check_out_tarde\",\n",
    "    \"check-out\": \"check_out\",\n",
    "    \"check out\": \"check_out\",\n",
    "    \"housekeeping\": \"limpieza_cuarto\",\n",
    "    \"room service\": \"servicio_cuarto\",\n",
    "    \"customer service\": \"servicio_cliente\",\n",
    "    \"concierge\": \"conserje\",\n",
    "    \"bellboy\": \"botones\",\n",
    "    \"valet parking\": \"valet_parking\",\n",
    "    \"self parking\": \"estacionamiento_autoservicio\",\n",
    "    \"parking lot\": \"estacionamiento\",\n",
    "    \"booking\": \"reserva\",\n",
    "    \"overbooking\": \"sobre_reserva\",\n",
    "    \"reservation\": \"reserva\",\n",
    "    \"upgrade\": \"mejora_habitacion\",\n",
    "    \"downgrade\": \"degradacion_habitacion\",\n",
    "    \"no-show\": \"no_show\",\n",
    "    \"walk-in\": \"llegada_sin_reserva\",\n",
    "    \"timeshare\": \"tiempos_compartidos\",\n",
    "    \"all inclusive\": \"todo_incluido\",\n",
    "    \"half board\": \"media_pension\",\n",
    "    \"full board\": \"pension_completa\",\n",
    "    \"resort fee\": \"cargo_resort\",\n",
    "    \"resort\": \"resort\",\n",
    "    \"security deposit\": \"deposito_garantia\",\n",
    "    \"refund\": \"reembolso\",\n",
    "    \"chargeback\": \"contracargo\",\n",
    "\n",
    "    # --- Habitaci√≥n / instalaciones ---\n",
    "    \"king bed\": \"cama_king\",\n",
    "    \"queen bed\": \"cama_queen\",\n",
    "    \"twin beds\": \"camas_individuales\",\n",
    "    \"double bed\": \"cama_matrimonial\",\n",
    "    \"sofa bed\": \"sofa_cama\",\n",
    "    \"bunk bed\": \"literas\",\n",
    "    \"extra bed\": \"cama_extra\",\n",
    "    \"rollaway bed\": \"cama_plegable\",\n",
    "    \"crib\": \"cuna\",\n",
    "    \"mattress\": \"colchon\",\n",
    "    \"pillow\": \"almohada\",\n",
    "    \"blanket\": \"cobija\",\n",
    "    \"linens\": \"sabanas\",\n",
    "    \"towels\": \"toallas\",\n",
    "    \"toilet paper\": \"papel_higienico\",\n",
    "    \"bathroom\": \"bano\",\n",
    "    \"bathtub\": \"banera\",\n",
    "    \"hot tub\": \"jacuzzi\",\n",
    "    \"jacuzzi\": \"jacuzzi\",\n",
    "    \"shower\": \"ducha\",\n",
    "    \"rain shower\": \"ducha_lluvia\",\n",
    "    \"water pressure\": \"presion_agua\",\n",
    "    \"air conditioning\": \"aire_acondicionado\",\n",
    "    \"heater\": \"calefaccion\",\n",
    "    \"safe box\": \"caja_fuerte\",\n",
    "    \"mini bar\": \"minibar\",\n",
    "    \"coffee maker\": \"cafetera\",\n",
    "    \"hair dryer\": \"secadora_cabello\",\n",
    "    \"iron\": \"plancha\",\n",
    "    \"iron board\": \"tabla_planchar\",\n",
    "    \"balcony\": \"balcon\",\n",
    "    \"terrace\": \"terraza\",\n",
    "    \"ocean view\": \"vista_mar\",\n",
    "    \"sea view\": \"vista_mar\",\n",
    "    \"garden view\": \"vista_jardin\",\n",
    "    \"city view\": \"vista_ciudad\",\n",
    "    \"soundproof\": \"insonorizado\",\n",
    "    \"elevator\": \"elevador\",\n",
    "    \"lift\": \"elevador\",\n",
    "    \"wheelchair\": \"silla_ruedas\",\n",
    "    \"wheelchair accessible\": \"acceso_silla_ruedas\",\n",
    "    \"roll-in shower\": \"ducha_accesible\",\n",
    "    \"accessible room\": \"habitacion_accesible\",\n",
    "\n",
    "    # --- Limpieza / plagas / olores ---\n",
    "    \"mold\": \"moho\",\n",
    "    \"mildew\": \"moho\",\n",
    "    \"leak\": \"fuga\",\n",
    "    \"leaking\": \"fuga\",\n",
    "    \"stain\": \"mancha\",\n",
    "    \"stains\": \"manchas\",\n",
    "    \"dusty\": \"polvoso\",\n",
    "    \"dirty\": \"sucio\",\n",
    "    \"smelly\": \"mal_olor\",\n",
    "    \"bad smell\": \"mal_olor\",\n",
    "    \"odor\": \"olor\",\n",
    "    \"bedbug\": \"chinche_cama\",\n",
    "    \"bedbugs\": \"chinches_cama\",\n",
    "    \"cockroach\": \"cucaracha\",\n",
    "    \"cockroaches\": \"cucarachas\",\n",
    "    \"ants\": \"hormigas\",\n",
    "    \"bugs\": \"insectos\",\n",
    "\n",
    "    # --- Personal / trato ---\n",
    "    \"staff\": \"personal\",\n",
    "    \"friendly\": \"amable\",\n",
    "    \"helpful\": \"servicial\",\n",
    "    \"rude\": \"grosero\",\n",
    "    \"impolite\": \"descortes\",\n",
    "    \"unprofessional\": \"poco_profesional\",\n",
    "    \"manager\": \"gerente\",\n",
    "    \"host\": \"anfitrion\",\n",
    "    \"hostess\": \"anfitriona\",\n",
    "    \"waiter\": \"mesero\",\n",
    "    \"waitress\": \"mesera\",\n",
    "    \"server\": \"mesero\",\n",
    "    \"bartender\": \"barman\",\n",
    "    \"lifeguard\": \"salvavidas\",\n",
    "\n",
    "    # --- Comida / bebida ---\n",
    "    \"breakfast\": \"desayuno\",\n",
    "    \"buffet\": \"buffet\",\n",
    "    \"brunch\": \"brunch\",\n",
    "    \"lunch\": \"comida\",\n",
    "    \"dinner\": \"cena\",\n",
    "    \"dessert\": \"postre\",\n",
    "    \"appetizer\": \"entrada\",\n",
    "    \"main course\": \"plato_fuerte\",\n",
    "    \"menu\": \"menu\",\n",
    "    \"dish\": \"platillo\",\n",
    "    \"portion\": \"porcion\",\n",
    "    \"taste\": \"sabor\",\n",
    "    \"tasteless\": \"insipido\",\n",
    "    \"delicious\": \"delicioso\",\n",
    "    \"undercooked\": \"crudo\",\n",
    "    \"overcooked\": \"sobre_cocido\",\n",
    "    \"cold food\": \"comida_fria\",\n",
    "    \"hot food\": \"comida_caliente\",\n",
    "    \"drink\": \"bebida\",\n",
    "    \"drinks\": \"bebidas\",\n",
    "    \"beverage\": \"bebida\",\n",
    "    \"beer\": \"cerveza\",\n",
    "    \"wine\": \"vino\",\n",
    "    \"cocktail\": \"coctel\",\n",
    "    \"water\": \"agua\",\n",
    "    \"ice\": \"hielo\",\n",
    "    \"inclusive drinks\": \"bebidas_incluidas\",\n",
    "\n",
    "    # --- Zonas / amenidades ---\n",
    "    \"pool\": \"alberca\",\n",
    "    \"infinity pool\": \"alberca_infinita\",\n",
    "    \"pool bar\": \"bar_alberca\",\n",
    "    \"beach\": \"playa\",\n",
    "    \"beach towel\": \"toalla_playa\",\n",
    "    \"private beach\": \"playa_privada\",\n",
    "    \"spa\": \"spa\",\n",
    "    \"sauna\": \"sauna\",\n",
    "    \"steam room\": \"vapor\",\n",
    "    \"gym\": \"gimnasio\",\n",
    "    \"kids club\": \"club_ninos\",\n",
    "    \"playground\": \"area_juegos\",\n",
    "    \"shuttle\": \"transporte\",\n",
    "    \"airport shuttle\": \"transporte_aeropuerto\",\n",
    "    \"uber\": \"uber\",\n",
    "    \"taxi\": \"taxi\",\n",
    "    \"tour\": \"tour\",\n",
    "    \"excursion\": \"excursion\",\n",
    "\n",
    "    # --- Tecnolog√≠a / internet ---\n",
    "    \"wifi\": \"wifi\",\n",
    "    \"wi-fi\": \"wifi\",\n",
    "    \"internet\": \"internet\",\n",
    "    \"signal\": \"senal\",\n",
    "    \"coverage\": \"cobertura\",\n",
    "    \"password\": \"contrasena\",\n",
    "    \"tv\": \"tv\",\n",
    "    \"smart tv\": \"smart_tv\",\n",
    "    \"channel\": \"canal\",\n",
    "    \"channels\": \"canales\",\n",
    "    \"remote control\": \"control_remoto\",\n",
    "    \"outlet\": \"contacto\",\n",
    "    \"power outlet\": \"contacto\",\n",
    "    \"charger\": \"cargador\",\n",
    "\n",
    "    # --- Precios / pagos / cargos ---\n",
    "    \"price\": \"precio\",\n",
    "    \"expensive\": \"caro\",\n",
    "    \"cheap\": \"barato\",\n",
    "    \"fee\": \"cargo\",\n",
    "    \"charge\": \"cargo\",\n",
    "    \"extra charge\": \"cargo_extra\",\n",
    "    \"hidden fee\": \"cargo_oculto\",\n",
    "    \"tip\": \"propina\",\n",
    "    \"tips\": \"propinas\",\n",
    "\n",
    "    # --- Ruido / seguridad ---\n",
    "    \"noise\": \"ruido\",\n",
    "    \"noisy\": \"ruidoso\",\n",
    "    \"thin walls\": \"paredes_delgadas\",\n",
    "    \"security\": \"seguridad\",\n",
    "    \"safe\": \"seguro\",\n",
    "    \"unsafe\": \"inseguro\",\n",
    "    \"theft\": \"robo\",\n",
    "    \"stolen\": \"robado\",\n",
    "\n",
    "    # --- Valoraciones r√°pidas ---\n",
    "    \"amazing\": \"increible\",\n",
    "    \"awesome\": \"increible\",\n",
    "    \"great\": \"genial\",\n",
    "    \"good\": \"bueno\",\n",
    "    \"ok\": \"regular\",\n",
    "    \"average\": \"promedio\",\n",
    "    \"bad\": \"malo\",\n",
    "    \"terrible\": \"terrible\",\n",
    "    \"awful\": \"horrible\",\n",
    "    \"horrible\": \"horrible\",\n",
    "    \"disappointing\": \"decepcionante\",\n",
    "    \"worth it\": \"vale_la_pena\",\n",
    "    \"not worth it\": \"no_vale_la_pena\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07a0e1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_es = set(SPACY_STOP_ES)\n",
    "negaciones = {\"no\",\"nunca\",\"jam√°s\",\"jamas\",\"ni\",\"sin\", \"tampoco\",\"nadie\",\"nada\",\n",
    "              \"ningun\",\"ning√∫n\",\"ninguno\",\"ninguna\",\"ningunos\",\"ningunas\"}\n",
    "NEG_SINGLE = {\"no\",\"nunca\",\"jamas\",\"jam√°s\",\"tampoco\",\"nadie\",\"nada\",\"ni\",\"sin\",\n",
    "              \"ningun\",\"ning√∫n\",\"ninguno\",\"ninguna\",\"ningunos\",\"ningunas\"}\n",
    "NEG_BIGRAMS = {(\"ya\",\"no\"),(\"ni\",\"siquiera\"),(\"para\",\"nada\"),(\"en\",\"absoluto\"),(\"sin\",\"que\")}\n",
    "stop_es_base = stop_es - negaciones\n",
    "stop_en_basic = {\"the\",\"a\",\"an\",\"and\",\"or\",\"but\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"to\",\"of\",\n",
    "                 \"in\",\"on\",\"at\",\"for\",\"with\",\"by\",\"about\",\"from\",\"it\",\"this\",\"that\",\"very\",\n",
    "                 \"really\",\"so\",\"just\",\"only\",\"my\",\"our\",\"your\",\"their\",\"we\",\"they\"}\n",
    "TIME_UNITS = {\"am\",\"pm\",\"hora\",\"horas\",\"minuto\",\"minutos\",\"segundo\",\"segundos\"}\n",
    "\n",
    "_re_url   = re.compile(r\"https?://\\S+|www\\.\\S+\", re.I)\n",
    "_re_email = re.compile(r\"\\b[\\w.+-]+@[\\w-]+\\.[\\w.-]+\\b\")\n",
    "_re_money = re.compile(r\"(?<!\\d)(\\$)\\s*\\d+[.,]?\\d*\")\n",
    "_re_num = re.compile(r\"(?<![\\w%$])\\d+[.,]?\\d*(?![\\w%$])\")\n",
    "_re_space = re.compile(r\"\\s+\")\n",
    "_re_risa  = re.compile(r\"j+a+(j+a+)+\", re.I)\n",
    "_re_repe  = re.compile(r\"(.)\\1{2,}\", re.UNICODE)\n",
    "_re_emoji = re.compile(\"[\" \"\\U0001F300-\\U0001FAFF\" \"\\U00002700-\\U000027BF\" \"\\U00002600-\\U000026FF\" \"]+\")\n",
    "_re_punct = re.compile(r\"[^\\w\\s%$]\")\n",
    "\n",
    "# Carga spaCy en espa√±ol, excluyendo lo que no se necesita\n",
    "nlp = spacy.load(\"es_core_news_sm\", exclude=[\"ner\",\"parser\",\"senter\",\"textcat\",\"tok2vec\"])\n",
    "BATCH, NPROC = 4000, 6\n",
    "\n",
    "# Emoji Lexicon\n",
    "emo_df  = pd.read_excel(\"Recursos Profe/Emojis lexicon.XLSX\")\n",
    "emo_df.columns = [str(c).strip().lower() for c in emo_df.columns]\n",
    "emo_cols = [\"anger\",\"anticipation\",\"disgust\",\"fear\",\"joy\",\"sadness\",\"surprise\",\"trust\"]\n",
    "sent_cols = [\"negative\",\"positive\"]\n",
    "emoji_list = emo_df[\"emoji\"].astype(str).tolist()\n",
    "weights_emotions = {\n",
    "    e: emo_df.set_index(\"emoji\")[e].to_dict() for e in emo_cols\n",
    "}\n",
    "weights_sent = {\n",
    "    s: emo_df.set_index(\"emoji\")[s].to_dict() for s in sent_cols\n",
    "}\n",
    "\n",
    "def emoji_weighted_features(raw_text: str):\n",
    "    # Cuenta ocurrencias por emoji (algunos pueden repetirse)\n",
    "    # Nota: .count() funciona bien para emojis de un caracter; si tuvieras secuencias, usa regex.\n",
    "    counts = {em: raw_text.count(em) for em in emoji_list}\n",
    "    total = sum(counts.values())\n",
    "\n",
    "    # Acumula puntajes ponderados por ocurrencia\n",
    "    feat = {}\n",
    "    # a) emociones b√°sicas\n",
    "    for e in emo_cols:\n",
    "        s = 0.0\n",
    "        wdict = weights_emotions[e]\n",
    "        for em, c in counts.items():\n",
    "            if c:\n",
    "                w = wdict.get(em, 0.0)\n",
    "                s += c * float(w)\n",
    "        feat[f\"emoji_{e}_score\"] = s\n",
    "\n",
    "    # b) polaridad agregada\n",
    "    pos = neg = 0.0\n",
    "    for em, c in counts.items():\n",
    "        if c:\n",
    "            pos += c * float(weights_sent[\"positive\"].get(em, 0.0))\n",
    "            neg += c * float(weights_sent[\"negative\"].get(em, 0.0))\n",
    "    feat[\"emoji_pos_score\"] = pos\n",
    "    feat[\"emoji_neg_score\"] = neg\n",
    "    feat[\"emoji_score\"] = pos - neg\n",
    "\n",
    "    # c) conteos crudos y densidades\n",
    "    feat[\"emoji_count\"] = int(total)\n",
    "    # normaliza por longitud para que no ‚Äúpremie‚Äù textos largos (opcional)\n",
    "    L = max(len(raw_text), 1)\n",
    "    feat[\"emoji_density\"] = total / L\n",
    "\n",
    "    return pd.Series(feat)\n",
    "\n",
    "# Lemtacizaci√≥n\n",
    "def rejoin_negations_after_lemma(text_lemmatized: str):\n",
    "    toks = text_lemmatized.split()\n",
    "    out = []; i = 0\n",
    "    while i < len(toks):\n",
    "        tk = toks[i]\n",
    "        # Bigramas\n",
    "        if i+1 < len(toks) and (tk, toks[i+1]) in NEG_BIGRAMS:\n",
    "            pair = f\"{tk}_{toks[i+1]}\"\n",
    "            if i+2 < len(toks) and toks[i+2] not in stop_es_base:\n",
    "                out.append(f\"{pair}_{toks[i+2]}\"); i += 3\n",
    "            else:\n",
    "                out.append(pair); i += 2\n",
    "            continue\n",
    "        # 'sin' solo\n",
    "        if tk == \"sin\":\n",
    "            if i+1 < len(toks) and toks[i+1] not in stop_es_base:\n",
    "                out.append(f\"sin_{toks[i+1]}\"); i += 2\n",
    "            else:\n",
    "                out.append(\"sin\"); i += 1\n",
    "            continue\n",
    "        # Un solo token (no, nunca, ni, tampoco, ning√∫n‚Ä¶)\n",
    "        if tk in NEG_SINGLE:\n",
    "            if i+1 < len(toks):\n",
    "                nxt = toks[i+1]\n",
    "                if (nxt not in stop_es_base and nxt != \"num\" and nxt not in TIME_UNITS):\n",
    "                    out.append(f\"{tk}_{nxt}\"); i += 2; continue\n",
    "            out.append(tk); i += 1; continue\n",
    "        out.append(tk); i += 1\n",
    "    return \" \".join(out)\n",
    "\n",
    "# Funciones de normalizaci√≥n\n",
    "def strip_accents(s): \n",
    "    nfkd = unicodedata.normalize(\"NFKD\", s)\n",
    "    return \"\".join(ch for ch in nfkd if not unicodedata.combining(ch))\n",
    "\n",
    "def reduce_repeated(text): return _re_repe.sub(r\"\\1\\1\", text)\n",
    "\n",
    "def replace_en_domain_words(text):\n",
    "    for en, es in en2es.items():\n",
    "        text = re.sub(rf\"\\b{re.escape(en)}\\b\", es, text, flags=re.I)\n",
    "    return text\n",
    "\n",
    "def normalize_text(\n",
    "    s: str,\n",
    "    *,\n",
    "    use_en2es: bool = True,\n",
    "    remove_stopwords: bool = True\n",
    ") -> str:\n",
    "    s = str(s).lower().replace(\"‚Äô\",\"'\").replace(\"‚Äú\",'\"').replace(\"‚Äù\",'\"')\n",
    "    s = _re_url.sub(\" url \", s); s = _re_email.sub(\" email \", s)\n",
    "    s = s.replace(\":)\", \" emoticon_positivo \").replace(\"(:\", \" emoticon_positivo \")\n",
    "    s = s.replace(\":(\", \" emoticon_negativo \").replace(\"):\", \" emoticon_negativo \")\n",
    "    # Nota: para el texto normalizado quitamos los emojis (se calculan features aparte sobre el texto crudo)\n",
    "    s = _re_emoji.sub(\" emoji \", s)\n",
    "    s = _re_risa.sub(\" jaja \", s); s = reduce_repeated(s)\n",
    "    s = _re_money.sub(\" dinero \", s); s = _re_num.sub(\" num \", s)\n",
    "    s = _re_punct.sub(\" \", s)\n",
    "    if use_en2es:\n",
    "        s = replace_en_domain_words(s)\n",
    "    s = _re_space.sub(\" \", s).strip()\n",
    "    if remove_stopwords:\n",
    "        toks = [t for t in s.split() if (t not in stop_es_base and t not in stop_en_basic)]\n",
    "        s = \" \".join(toks)\n",
    "    return s\n",
    "\n",
    "def maybe_lemmatize(texts, do_lemma: bool = True):\n",
    "    if not do_lemma:\n",
    "        # Si no lematizamos, devolvemos tal cual\n",
    "        return [t for t in texts]\n",
    "    lemmas_local = []\n",
    "    for doc in nlp.pipe(texts, batch_size=BATCH, n_process=NPROC):\n",
    "        lemmas_local.append(\" \".join(t.lemma_ for t in doc))\n",
    "    return lemmas_local\n",
    "\n",
    "def maybe_strip_accents(texts, do_strip: bool = True):\n",
    "    if not do_strip:\n",
    "        return texts\n",
    "    return [strip_accents(t) for t in texts]\n",
    "\n",
    "def maybe_join_negations(texts, do_join: bool = True):\n",
    "    if not do_join:\n",
    "        return texts\n",
    "    return [rejoin_negations_after_lemma(t) for t in texts]\n",
    "\n",
    "def add_emoji_features(df_out, do_emoji_feats: bool = True):\n",
    "    if not do_emoji_feats:\n",
    "        return df_out\n",
    "    feats = df_out[\"text\"].apply(emoji_weighted_features)\n",
    "    df_out = pd.concat([df_out, feats], axis=1)\n",
    "    return df_out\n",
    "\n",
    "def add_sel_features(df_out, do_sel_feats: bool = True):\n",
    "    if not do_sel_feats:\n",
    "        return df_out\n",
    "    sel_feats = df_out[\"text_norm\"].apply(sel_features)\n",
    "    df_out = pd.concat([df_out, sel_feats.rename(columns={\"sel_score\":\"sel_score\"})], axis=1)\n",
    "    return df_out\n",
    "\n",
    "# SEL\n",
    "sel = pd.read_csv(\"Recursos Profe/SEL_full.txt\", sep=r\"\\s*\\t\\s*\", engine=\"python\")\n",
    "sel[\"key\"] = sel[\"Palabra\"].str.lower().apply(strip_accents)\n",
    "pfa = dict(zip(sel[\"key\"], sel[\"PFA\"]))\n",
    "cat = dict(zip(sel[\"key\"], sel[\"Categor√≠a\"].str.lower()))\n",
    "emociones = sorted(sel[\"Categor√≠a\"].str.lower().unique())\n",
    "\n",
    "def sel_features(text_norm):\n",
    "    toks = text_norm.split()\n",
    "    score = 0.0\n",
    "    counts = {e: 0 for e in emociones}\n",
    "    for t in toks:\n",
    "        w = t.replace(\"no_\",\"\"); inv = t.startswith(\"no_\")\n",
    "        if w in pfa: score += (-pfa[w] if inv else pfa[w])\n",
    "        if w in cat: counts[cat[w]] += (-1 if inv else 1)\n",
    "    counts[\"sel_score\"] = score\n",
    "    return pd.Series(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493dbd9a",
   "metadata": {},
   "source": [
    "Generar multiples archivos con el pre procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ade8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define multiples formas de normalizar el texto y agregar features.\n",
    "VARIANTS = [\n",
    "    # Todo activado\n",
    "    dict(name=\"all_on\",\n",
    "         strip_accents=True, en2es=True, stopwords=True, lemma=True, neg_join=True, emoji_feats=True, sel_feats=True),\n",
    "\n",
    "    # Uno a uno desactivados\n",
    "    dict(name=\"no_accents\",\n",
    "         strip_accents=False, en2es=True, stopwords=True, lemma=True, neg_join=True, emoji_feats=True, sel_feats=True),\n",
    "    dict(name=\"no_en2es\",\n",
    "         strip_accents=True, en2es=False, stopwords=True, lemma=True, neg_join=True, emoji_feats=True, sel_feats=True),\n",
    "    dict(name=\"no_stopwords\",\n",
    "         strip_accents=True, en2es=True, stopwords=False, lemma=True, neg_join=True, emoji_feats=True, sel_feats=True),\n",
    "    dict(name=\"no_lemma\",\n",
    "         strip_accents=True, en2es=True, stopwords=True, lemma=False, neg_join=True, emoji_feats=True, sel_feats=True),\n",
    "    dict(name=\"no_negjoin\",\n",
    "         strip_accents=True, en2es=True, stopwords=True, lemma=True, neg_join=False, emoji_feats=True, sel_feats=True),\n",
    "    dict(name=\"no_emoji_feats\",\n",
    "         strip_accents=True, en2es=True, stopwords=True, lemma=True, neg_join=True, emoji_feats=False, sel_feats=True),\n",
    "    dict(name=\"no_sel\",\n",
    "         strip_accents=True, en2es=True, stopwords=True, lemma=True, neg_join=True, emoji_feats=True, sel_feats=False),\n",
    "\n",
    "    # Limpieza ligera sin extras\n",
    "    dict(name=\"minimal\",\n",
    "         strip_accents=False, en2es=False, stopwords=False, lemma=False, neg_join=False, emoji_feats=False, sel_feats=False),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16facb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando variante: all_on -> {'name': 'all_on', 'strip_accents': True, 'en2es': True, 'stopwords': True, 'lemma': True, 'neg_join': True, 'emoji_feats': True, 'sel_feats': True}\n",
      "Generando variante: no_accents -> {'name': 'no_accents', 'strip_accents': False, 'en2es': True, 'stopwords': True, 'lemma': True, 'neg_join': True, 'emoji_feats': True, 'sel_feats': True}\n",
      "Generando variante: no_en2es -> {'name': 'no_en2es', 'strip_accents': True, 'en2es': False, 'stopwords': True, 'lemma': True, 'neg_join': True, 'emoji_feats': True, 'sel_feats': True}\n",
      "Generando variante: no_stopwords -> {'name': 'no_stopwords', 'strip_accents': True, 'en2es': True, 'stopwords': False, 'lemma': True, 'neg_join': True, 'emoji_feats': True, 'sel_feats': True}\n",
      "Generando variante: no_lemma -> {'name': 'no_lemma', 'strip_accents': True, 'en2es': True, 'stopwords': True, 'lemma': False, 'neg_join': True, 'emoji_feats': True, 'sel_feats': True}\n",
      "Generando variante: no_negjoin -> {'name': 'no_negjoin', 'strip_accents': True, 'en2es': True, 'stopwords': True, 'lemma': True, 'neg_join': False, 'emoji_feats': True, 'sel_feats': True}\n",
      "Generando variante: no_emoji_feats -> {'name': 'no_emoji_feats', 'strip_accents': True, 'en2es': True, 'stopwords': True, 'lemma': True, 'neg_join': True, 'emoji_feats': False, 'sel_feats': True}\n",
      "Generando variante: no_sel -> {'name': 'no_sel', 'strip_accents': True, 'en2es': True, 'stopwords': True, 'lemma': True, 'neg_join': True, 'emoji_feats': True, 'sel_feats': False}\n",
      "Generando variante: minimal -> {'name': 'minimal', 'strip_accents': False, 'en2es': False, 'stopwords': False, 'lemma': False, 'neg_join': False, 'emoji_feats': False, 'sel_feats': False}\n"
     ]
    }
   ],
   "source": [
    "# Se define donde se guardaran los archivos procesados\n",
    "BASE_OUT = \"Archivos_Procesados/Rest_Mex_2022_preprocessed__\"\n",
    "\n",
    "for cfg in VARIANTS:\n",
    "    print(f\"Generando variante: {cfg['name']} -> {cfg}\")\n",
    "    df_var = df.copy()\n",
    "\n",
    "    # Normalizaci√≥n con switches (en2es + stopwords)\n",
    "    df_var[\"text_clean_tmp\"] = df_var[\"text\"].apply(\n",
    "        lambda s: normalize_text(s, use_en2es=cfg[\"en2es\"], remove_stopwords=cfg[\"stopwords\"])\n",
    "    )\n",
    "\n",
    "    # Lematizaci√≥n opcional\n",
    "    lemmas_local = maybe_lemmatize(df_var[\"text_clean_tmp\"].tolist(), do_lemma=cfg[\"lemma\"])\n",
    "\n",
    "    # Acentos opcionales\n",
    "    lemmas_local = maybe_strip_accents(lemmas_local, do_strip=cfg[\"strip_accents\"])\n",
    "\n",
    "    # Uni√≥n de negaciones opcional\n",
    "    lemmas_local = maybe_join_negations(lemmas_local, do_join=cfg[\"neg_join\"])\n",
    "\n",
    "    df_var[\"text_norm\"] = lemmas_local\n",
    "    df_var.drop(columns=[\"text_clean_tmp\"], inplace=True)\n",
    "\n",
    "    # Features opcionales emojis / SEL\n",
    "    df_var = add_emoji_features(df_var, do_emoji_feats=cfg[\"emoji_feats\"])\n",
    "    df_var = add_sel_features(df_var, do_sel_feats=cfg[\"sel_feats\"])\n",
    "\n",
    "    # Guarda archivo\n",
    "    out_file = f\"{BASE_OUT}{cfg['name']}.xlsx\"\n",
    "    df_var.to_excel(out_file, index=False, engine=\"openpyxl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c884353c",
   "metadata": {},
   "source": [
    "# **Parte 2 Alberto**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a31787",
   "metadata": {},
   "source": [
    "**Configurar columnas y asegurar split 80/20**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2eb7210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack, csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ec14409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Title', 'Opinion', 'Polarity', 'Attraction', 'text', 'target',\n",
      "       'text_norm', 'emoji_anger_score', 'emoji_anticipation_score',\n",
      "       'emoji_disgust_score', 'emoji_fear_score', 'emoji_joy_score',\n",
      "       'emoji_sadness_score', 'emoji_surprise_score', 'emoji_trust_score',\n",
      "       'emoji_pos_score', 'emoji_neg_score', 'emoji_score', 'emoji_count',\n",
      "       'emoji_density', 'sel_score', 'alegr√≠a', 'enojo', 'miedo', 'repulsi√≥n',\n",
      "       'sorpresa', 'tristeza'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P√©simo lugar Piensen dos veces antes de ir a e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No vayas a lugar de Eddie Cuatro de nosotros f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mala relaci√≥n calidad-precio seguir√© corta y s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Minusv√°lido? ¬°No te alojes aqu√≠! Al reservar u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Es una porqueria no pierdan su tiempo No pierd...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  P√©simo lugar Piensen dos veces antes de ir a e...       1\n",
       "1  No vayas a lugar de Eddie Cuatro de nosotros f...       1\n",
       "2  Mala relaci√≥n calidad-precio seguir√© corta y s...       1\n",
       "3  Minusv√°lido? ¬°No te alojes aqu√≠! Al reservar u...       1\n",
       "4  Es una porqueria no pierdan su tiempo No pierd...       1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df.columns)\n",
    "\n",
    "# Ajustar nombres seg√∫n lo que ya creaste\n",
    "TEXT_COL = 'text'      # columna con Title + Opinion concatenado\n",
    "LABEL_COL = 'target'   # o 'Polarity' \n",
    "\n",
    "display(df[[TEXT_COL, LABEL_COL]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3badfaf",
   "metadata": {},
   "source": [
    "**Split 80/20 con shuffle=True y random_state=0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "506f7a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tama√±o train: 24169\n",
      "Tama√±o test: 6043\n"
     ]
    }
   ],
   "source": [
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    df[TEXT_COL],\n",
    "    df[LABEL_COL],\n",
    "    test_size=0.2,\n",
    "    shuffle=True,\n",
    "    random_state=0,\n",
    "    stratify=df[LABEL_COL]  #por el desbalance\n",
    ")\n",
    "\n",
    "print(\"Tama√±o train:\", len(X_train_text))\n",
    "print(\"Tama√±o test:\", len(X_test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1048ed31",
   "metadata": {},
   "source": [
    "**Funciones de vectorizaci√≥n (binaria, frecuencia, TF-IDF)**  \n",
    "Estas funciones:\n",
    "\n",
    "Transforman texto ‚Üí matriz num√©rica.\n",
    "\n",
    "Mantienen el mismo split porque el vectorizador se ajusta SOLO con X_train_text y luego se aplica a X_test_text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3d89506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_binary(X_train_text, X_test_text, **kwargs):\n",
    "    \"\"\"\n",
    "    Representaci√≥n binarizada: 1 si la palabra aparece, 0 si no.\n",
    "    \"\"\"\n",
    "    vect_bin = CountVectorizer(binary=True, **kwargs)\n",
    "    X_train_bin = vect_bin.fit_transform(X_train_text)\n",
    "    X_test_bin = vect_bin.transform(X_test_text)\n",
    "    return X_train_bin, X_test_bin, vect_bin\n",
    "\n",
    "def vectorize_frequency(X_train_text, X_test_text, **kwargs):\n",
    "    \"\"\"\n",
    "    Representaci√≥n por frecuencia: conteo de ocurrencias de cada t√©rmino.\n",
    "    \"\"\"\n",
    "    vect_freq = CountVectorizer(binary=False, **kwargs)\n",
    "    X_train_freq = vect_freq.fit_transform(X_train_text)\n",
    "    X_test_freq = vect_freq.transform(X_test_text)\n",
    "    return X_train_freq, X_test_freq, vect_freq\n",
    "\n",
    "def vectorize_tfidf(X_train_text, X_test_text, **kwargs):\n",
    "    \"\"\"\n",
    "    Representaci√≥n TF-IDF: pondera t√©rminos seg√∫n importancia en documentos.\n",
    "    \"\"\"\n",
    "    vect_tfidf = TfidfVectorizer(**kwargs)\n",
    "    X_train_tfidf = vect_tfidf.fit_transform(X_train_text)\n",
    "    X_test_tfidf = vect_tfidf.transform(X_test_text)\n",
    "    return X_train_tfidf, X_test_tfidf, vect_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c7de5d",
   "metadata": {},
   "source": [
    "**Ejemplo r√°pido para generar las tres:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d9d1121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binaria: (24169, 45691) (6043, 45691)\n",
      "Frecuencia: (24169, 45691) (6043, 45691)\n",
      "TF-IDF: (24169, 45691) (6043, 45691)\n"
     ]
    }
   ],
   "source": [
    "X_train_bin, X_test_bin, vect_bin = vectorize_binary(X_train_text, X_test_text)\n",
    "X_train_freq, X_test_freq, vect_freq = vectorize_frequency(X_train_text, X_test_text)\n",
    "X_train_tfidf, X_test_tfidf, vect_tfidf = vectorize_tfidf(X_train_text, X_test_text)\n",
    "\n",
    "print(\"Binaria:\", X_train_bin.shape, X_test_bin.shape)\n",
    "print(\"Frecuencia:\", X_train_freq.shape, X_test_freq.shape)\n",
    "print(\"TF-IDF:\", X_train_tfidf.shape, X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99268d49",
   "metadata": {},
   "source": [
    "**L√©xicos de sentimiento (features extra)**  \n",
    "Aqu√≠ agregamos conteo de palabras positivas y negativas por documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7165340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo simple de l√©xicos (AMPL√çA ESTO para la pr√°ctica real)\n",
    "positive_lexicon = {\n",
    "    \"bueno\",\"excelente\",\"perfecto\",\"maravilloso\",\"incre√≠ble\",\"bonito\",\"agradable\",\n",
    "    \"recomendado\",\"limpio\",\"r√°pido\",\"delicioso\",\"rico\",\"amable\",\"fant√°stico\",\"genial\"\n",
    "}\n",
    "\n",
    "negative_lexicon = {\n",
    "    \"malo\",\"terrible\",\"horrible\",\"p√©simo\",\"lento\",\"sucio\",\"caro\",\n",
    "    \"decepcionante\",\"desagradable\",\"asqueroso\",\"fr√≠o\",\"tardado\",\"estresante\",\"feo\"\n",
    "}\n",
    "\n",
    "def count_sentiment_words(text, pos_lex=positive_lexicon, neg_lex=negative_lexicon):\n",
    "    tokens = text.split()\n",
    "    pos_count = sum(1 for t in tokens if t in pos_lex)\n",
    "    neg_count = sum(1 for t in tokens if t in neg_lex)\n",
    "    return pos_count, neg_count\n",
    "\n",
    "def build_lexicon_features(text_series):\n",
    "    \"\"\"\n",
    "    Regresa una matriz numpy de shape (n_docs, 2):\n",
    "    [conteo_pos, conteo_neg]\n",
    "    \"\"\"\n",
    "    feats = [count_sentiment_words(str(t)) for t in text_series]\n",
    "    return np.array(feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70d3ba0",
   "metadata": {},
   "source": [
    "**prueba**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8eec752b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L√©xicos train: (24169, 2)\n",
      "L√©xicos test: (6043, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 0],\n",
       "       [1, 0],\n",
       "       [0, 0],\n",
       "       [5, 1]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lex_train = build_lexicon_features(X_train_text)\n",
    "lex_test = build_lexicon_features(X_test_text)\n",
    "\n",
    "print(\"L√©xicos train:\", lex_train.shape)\n",
    "print(\"L√©xicos test:\", lex_test.shape)\n",
    "lex_train[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a059187",
   "metadata": {},
   "source": [
    "**Emojis / Emoticonos (features extra)**  \n",
    "Features t√≠picos:\n",
    "\n",
    "Conteo de emojis/emoticonos positivos.\n",
    "\n",
    "Conteo de emojis/emoticonos negativos.\n",
    "\n",
    "(Opcional) Conteo de signos de exclamaci√≥n para intensidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04cd9db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emojis positivos detectados: {'üé∏', 'üí§', 'üåπ', 'üèä', 'üôå', 'üë¶', 'üòú', 'üêô', 'ü§©', 'üò¨', 'ü§ò', 'üéº', 'ü™Ç', 'üê≤', '‚ô°', 'üòÄ', '‚ò∫', 'üèÑ', 'üòç', 'üç≠', 'ü¶™', 'üéâ', 'ü•Ç', '‚úå', 'üåû', 'üíß', 'ü§ç', 'üëß', '‚ôÄ', 'üèø', 'üèª', 'üç∫', 'ü§™', 'üçæ', 'üéÇ', 'üçª', 'üå≠', '‚òÄ', 'üîü', 'üëè', 'üòä', 'üéá', 'ü•ó', 'üë©', 'üç∏', 'ü§£', 'üòÑ', 'üëØ', 'üå¥', 'ü•∞', 'üé∂', 'üòÉ', 'ü•§', 'üëå', 'üé•', 'üòö', 'ü¶é', 'üëê', 'üî•', 'ü§ô', 'üåª', 'üòå', 'üèñ', 'üéà', 'ü•∫', '‚úî', 'üôÇ', 'üëç', '‚úã', 'üåÖ', '‚òπ', 'üçü', 'üé≠', 'üëä', 'ü¶ê', 'üåÆ', 'üòÜ', 'üñ§', 'üåà', 'üç≤', 'ü§§', 'üè°', 'üíÜ', '‚ôß', '‚ôÇ', 'üíô', '‚úÖ', 'üè©', 'üôè', 'üë™', 'üíì', 'üòâ', '‚ú®', 'üèÖ', 'üëì', 'üîô', 'ü§Ω', 'üçî', 'üê∑', 'üé§', 'üòã', 'üò∏', 'üåé', '‚ô•', 'üò™', 'üçØ', 'üòá', 'üòà', 'üíê', '‚ù§', 'üôà', 'üï∂', 'üíÅ', 'üéä', 'üò≠', 'üçï', 'üçπ', 'üèº', '‚õ±', 'üôã', 'üíñ', 'üèΩ', 'üëÖ', 'üèì', 'üåä', 'üëô', 'üòë', 'üíé', 'üòÇ', 'üíï', 'üòù', 'üçù', 'üå∑', '‚ùÑ', 'üåã', '‚öΩ', '‚ùó', 'üèê', 'üåö', 'üôä', 'üíõ', 'üå∫', 'üëÄ', 'üí™', 'üë®', 'üíó', 'üòÅ', 'üç£', 'üèæ', 'üò∑', 'üòñ', 'ü§ì', 'üòé', 'üôÉ', 'üèÜ', 'üîÜ', 'ü§ü', 'üòÖ', 'ü§†', 'üíØ', 'üíú', 'ü§ó', 'üëé', 'üò¢', 'ü¶©', 'ü•ò', 'ü§î', 'üòõ', 'ü¶û', 'üèù', 'üçú', 'üòò', 'üò±', 'üò¶', 'üëã', 'ü•≥'}\n",
      "Emojis negativos detectados: {'üò°', 'üôÅ', 'üò†', 'üòî', 'ü§ö', 'üòì', 'üò≥'}\n",
      "Emoji feats train: (24169, 3)\n",
      "Emoji feats test: (6043, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_pattern = re.compile(\n",
    "    r'['\n",
    "    r'\\U0001F300-\\U0001F5FF'  # s√≠mbolos y pictogramas\n",
    "    r'\\U0001F600-\\U0001F64F'  # emoticonos\n",
    "    r'\\U0001F680-\\U0001F6FF'  # transporte y mapas\n",
    "    r'\\U0001F700-\\U0001F77F'\n",
    "    r'\\U0001F780-\\U0001F7FF'\n",
    "    r'\\U0001F800-\\U0001F8FF'\n",
    "    r'\\U0001F900-\\U0001F9FF'\n",
    "    r'\\U0001FA00-\\U0001FAFF'\n",
    "    r'\\u2600-\\u26FF'          # s√≠mbolos varios\n",
    "    r'\\u2700-\\u27BF'\n",
    "    r']'\n",
    ")\n",
    "\n",
    "def extract_emojis(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    return emoji_pattern.findall(text)\n",
    "\n",
    "# Definir positivo / negativo \n",
    "# 1 = muy negativo, 2 = negativo, 3 = neutro, 4 = positivo, 5 = muy positivo\n",
    "POS_LABELS = {4, 5}\n",
    "NEG_LABELS = {1, 2}\n",
    "\n",
    "pos_counts = {}\n",
    "neg_counts = {}\n",
    "\n",
    "for txt, label in zip(df[TEXT_COL], df[LABEL_COL]):\n",
    "    emojis = extract_emojis(txt)\n",
    "    if not emojis:\n",
    "        continue\n",
    "\n",
    "    # normalizamos label como string\n",
    "    lab = str(label).strip()\n",
    "\n",
    "    # conjuntos ya como strings para comparar\n",
    "    pos_set = {str(l) for l in POS_LABELS}\n",
    "    neg_set = {str(l) for l in NEG_LABELS}\n",
    "\n",
    "    if lab in pos_set:\n",
    "        for e in emojis:\n",
    "            pos_counts[e] = pos_counts.get(e, 0) + 1\n",
    "    elif lab in neg_set:\n",
    "        for e in emojis:\n",
    "            neg_counts[e] = neg_counts.get(e, 0) + 1\n",
    "\n",
    "# Emojis clasificados seg√∫n en qu√© clase aparecen m√°s\n",
    "positive_emojis = {e for e in pos_counts if pos_counts[e] > neg_counts.get(e, 0)}\n",
    "negative_emojis = {e for e in neg_counts if neg_counts[e] > pos_counts.get(e, 0)}\n",
    "\n",
    "print(\"Emojis positivos detectados:\", positive_emojis)\n",
    "print(\"Emojis negativos detectados:\", negative_emojis)\n",
    "\n",
    "# Emoticonos comunes \n",
    "positive_emoticons = {\":)\", \":-)\", \":D\", \"(:\", \"=)\", \";)\", \";-)\", \":')\"}\n",
    "negative_emoticons = {\":(\", \":-(\", \"):\", \"='(\", \":'(\", \"D:\", \">:(\", \":-/\"}\n",
    "\n",
    "# Funciones de features\n",
    "def emoji_emoticon_features(text):\n",
    "    t = str(text)\n",
    "\n",
    "    # conteo de emojis seg√∫n los sets aprendidos\n",
    "    pos = sum(ch in positive_emojis for ch in t)\n",
    "    neg = sum(ch in negative_emojis for ch in t)\n",
    "\n",
    "    # conteo de emoticonos\n",
    "    for emo in positive_emoticons:\n",
    "        pos += t.count(emo)\n",
    "    for emo in negative_emoticons:\n",
    "        neg += t.count(emo)\n",
    "\n",
    "    # intensidad con signos de exclamaci√≥n\n",
    "    exclam = t.count(\"!\")\n",
    "\n",
    "    return pos, neg, exclam\n",
    "\n",
    "def build_emoji_features(text_series):\n",
    "    \"\"\"\n",
    "    Regresa matriz (n_docs, 3):\n",
    "    [emoji_pos+emo_pos, emoji_neg+emo_neg, exclamaciones]\n",
    "    \"\"\"\n",
    "    feats = [emoji_emoticon_features(t) for t in text_series]\n",
    "    return np.array(feats)\n",
    "\n",
    "# Generar features para train/test\n",
    "emoji_train = build_emoji_features(X_train_text)\n",
    "emoji_test = build_emoji_features(X_test_text)\n",
    "\n",
    "print(\"Emoji feats train:\", emoji_train.shape)\n",
    "print(\"Emoji feats test:\", emoji_test.shape)\n",
    "emoji_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d426ab",
   "metadata": {},
   "source": [
    "**Combinar representaciones + features extra**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "156e5c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF simple: (24169, 45691)\n",
      "TF-IDF + extras: (24169, 45696)\n"
     ]
    }
   ],
   "source": [
    "def add_extra_features(X_train_base, X_test_base, extra_train, extra_test):\n",
    "    \"\"\"\n",
    "    Concatena la matriz dispersa base con features densos extra.\n",
    "    \"\"\"\n",
    "    extra_train_sparse = csr_matrix(extra_train)\n",
    "    extra_test_sparse = csr_matrix(extra_test)\n",
    "    \n",
    "    X_train_comb = hstack([X_train_base, extra_train_sparse])\n",
    "    X_test_comb = hstack([X_test_base, extra_test_sparse])\n",
    "    \n",
    "    return X_train_comb, X_test_comb\n",
    "\n",
    "# Ejemplo: TF-IDF + l√©xicos + emojis\n",
    "extra_train = np.hstack([lex_train, emoji_train])\n",
    "extra_test = np.hstack([lex_test, emoji_test])\n",
    "\n",
    "X_train_tfidf_ext, X_test_tfidf_ext = add_extra_features(\n",
    "    X_train_tfidf, X_test_tfidf,\n",
    "    extra_train, extra_test\n",
    ")\n",
    "\n",
    "print(\"TF-IDF simple:\", X_train_tfidf.shape)\n",
    "print(\"TF-IDF + extras:\", X_train_tfidf_ext.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e73051",
   "metadata": {},
   "source": [
    "# **Tabla/resumen**  \n",
    "| Representaci√≥n      | Descripci√≥n                                                                 | Ventajas                                                   | Desventajas                                                    |\n",
    "|---------------------|-----------------------------------------------------------------------------|------------------------------------------------------------|----------------------------------------------------------------|\n",
    "| Binarizada          | Cada t√©rmino se representa con 0/1 seg√∫n aparezca en el documento.         | Simple, reduce impacto de repeticiones, √∫til con muchos docs. | Pierde informaci√≥n de frecuencia; menos expresiva.            |\n",
    "| Frecuencia (BoW)    | Cada t√©rmino es el n√∫mero de veces que aparece en el documento.            | F√°cil de interpretar, buena base para muchos modelos.      | Favorece palabras muy frecuentes; no considera importancia global. |\n",
    "| TF-IDF              | Pondera frecuencia local vs. frecuencia global del t√©rmino.                | Resalta t√©rminos relevantes; muy usada en IR y NLP cl√°sico.| M√°s compleja; puede sobreajustar si no se regula.              |\n",
    "| L√©xicos de sentimiento (extra) | Features num√©ricos con conteo de palabras positivas/negativas.          | Introduce conocimiento ling√º√≠stico expl√≠cito.             | Depende de la calidad/cobertura del l√©xico; sensible al dominio. |\n",
    "| Emojis/Emoticonos (extra)      | Conteo de emojis/emoticonos positivos/negativos e intensidad (!).       | Captura se√±ales afectivas t√≠picas de redes/comentarios.   | No siempre aparecen; puede ser ruidoso si se usa solo.         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6dee45",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60e1009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuci√≥n de clases en el dataset completo:\n",
      "target\n",
      "1      547\n",
      "2      730\n",
      "3     2121\n",
      "4     5878\n",
      "5    20936\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Tama√±o train: 24169\n",
      "Tama√±o test: 6043\n",
      "Distribuci√≥n en train: [    0   438   584  1697  4702 16748]\n",
      "Distribuci√≥n en test: [   0  109  146  424 1176 4188]\n",
      "================================================================================\n",
      "COMIENZO DE EXPERIMENTACI√ìN CON 5-FOLD CV\n",
      "================================================================================\n",
      "\n",
      "--- Probando representaci√≥n: CountBinary ---\n",
      "  Modelo: LogisticRegression | Balanceo: none\n",
      "    Error: 'test_f1_macro'\n",
      "  Modelo: LogisticRegression | Balanceo: oversample\n",
      "    Error: 'test_f1_macro'\n",
      "  Modelo: LogisticRegression | Balanceo: undersample\n",
      "    Error: 'test_f1_macro'\n",
      "  Modelo: LogisticRegression | Balanceo: smote\n",
      "    Error: 'test_f1_macro'\n",
      "  Modelo: RandomForest | Balanceo: none\n",
      "    Error: 'test_f1_macro'\n",
      "  Modelo: RandomForest | Balanceo: oversample\n",
      "    Error: 'test_f1_macro'\n",
      "  Modelo: RandomForest | Balanceo: undersample\n",
      "    Error: 'test_f1_macro'\n",
      "  Modelo: RandomForest | Balanceo: smote\n",
      "    Error: 'test_f1_macro'\n",
      "  Modelo: SVM | Balanceo: none\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_validate, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Para manejo de desbalanceo\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# Cargar el dataset preprocesado\n",
    "df = pd.read_excel('Rest_Mex_2022_preprocessed.xlsx')\n",
    "\n",
    "# Configurar columnas\n",
    "TEXT_COL = 'text_norm'\n",
    "LABEL_COL = 'target'\n",
    "\n",
    "print(\"Distribuci√≥n de clases en el dataset completo:\")\n",
    "print(df[LABEL_COL].value_counts().sort_index())\n",
    "\n",
    "# Split 80/20\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    df[TEXT_COL],\n",
    "    df[LABEL_COL],\n",
    "    test_size=0.2,\n",
    "    shuffle=True,\n",
    "    random_state=0,\n",
    "    stratify=df[LABEL_COL]\n",
    ")\n",
    "\n",
    "print(f\"\\nTama√±o train: {len(X_train_text)}\")\n",
    "print(f\"Tama√±o test: {len(X_test_text)}\")\n",
    "print(\"Distribuci√≥n en train:\", np.bincount(y_train))\n",
    "print(\"Distribuci√≥n en test:\", np.bincount(y_test))\n",
    "\n",
    "# =============================================================================\n",
    "# DIFERENTES REPRESENTACIONES DE TEXTO\n",
    "# =============================================================================\n",
    "\n",
    "def create_vectorizers():\n",
    "    \"\"\"Crea diferentes representaciones vectoriales\"\"\"\n",
    "    vectorizers = {\n",
    "        'CountBinary': CountVectorizer(binary=True, max_features=5000),\n",
    "        'CountFreq': CountVectorizer(binary=False, max_features=5000),\n",
    "        'TFIDF': TfidfVectorizer(max_features=5000),\n",
    "        'TFIDF_ngrams': TfidfVectorizer(ngram_range=(1, 2), max_features=5000)\n",
    "    }\n",
    "    return vectorizers\n",
    "\n",
    "# =============================================================================\n",
    "# CARACTER√çSTICAS L√âXICAS SEL\n",
    "# =============================================================================\n",
    "\n",
    "def load_sel():\n",
    "    lexicon_sel = {}\n",
    "    try:\n",
    "        input_file = open('Recursos Profe/SEL_full.txt', 'r', encoding='utf-8')\n",
    "        for line in input_file:\n",
    "            palabras = line.split(\"\\t\")\n",
    "            if len(palabras) >= 7:\n",
    "                palabras[6] = re.sub('\\n', '', palabras[6])\n",
    "                pair = (palabras[6], palabras[5])\n",
    "                if palabras[0] not in lexicon_sel:\n",
    "                    lista = [pair]\n",
    "                    lexicon_sel[palabras[0]] = lista\n",
    "                else:\n",
    "                    lexicon_sel[palabras[0]].append(pair)\n",
    "        input_file.close()\n",
    "        if 'Palabra' in lexicon_sel:\n",
    "            del lexicon_sel['Palabra']\n",
    "    except Exception as e:\n",
    "        print(f\"Error cargando SEL: {e}\")\n",
    "        lexicon_sel = {}\n",
    "    return lexicon_sel\n",
    "\n",
    "def getSELFeatures(cadenas, lexicon_sel):\n",
    "    features = []\n",
    "    for cadena in cadenas:\n",
    "        valores = {emocion: 0.0 for emocion in ['alegria', 'tristeza', 'enojo', 'repulsion', 'miedo', 'sorpresa']}\n",
    "        \n",
    "        cadena_palabras = re.split('\\s+', cadena)\n",
    "        \n",
    "        for palabra in cadena_palabras:\n",
    "            if palabra in lexicon_sel:\n",
    "                caracteristicas = lexicon_sel[palabra]\n",
    "                for emocion, valor in caracteristicas:\n",
    "                    emocion_key = emocion.lower().strip()\n",
    "                    if emocion_key in valores:\n",
    "                        valores[emocion_key] += float(valor)\n",
    "        \n",
    "        # Caracter√≠sticas adicionales\n",
    "        valores['acumuladopositivo'] = valores['alegria'] + valores['sorpresa']\n",
    "        valores['acumuladonegative'] = valores['enojo'] + valores['miedo'] + valores['repulsion'] + valores['tristeza']\n",
    "        \n",
    "        features.append(valores)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Cargar l√©xico SEL\n",
    "lexicon_sel = load_sel()\n",
    "sel_cols = ['alegria', 'tristeza', 'enojo', 'repulsion', 'miedo', 'sorpresa', 'acumuladopositivo', 'acumuladonegative']\n",
    "\n",
    "# =============================================================================\n",
    "# ESTRATEGIAS DE BALANCEO\n",
    "# =============================================================================\n",
    "\n",
    "balance_strategies = {\n",
    "    'none': None,\n",
    "    'oversample': RandomOverSampler(random_state=0),\n",
    "    'undersample': RandomUnderSampler(random_state=0),\n",
    "    'smote': SMOTE(random_state=0)\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# MODELOS E HIPERPAR√ÅMETROS\n",
    "# =============================================================================\n",
    "\n",
    "models_config = {\n",
    "    'LogisticRegression': {\n",
    "        'model': LogisticRegression(max_iter=10000, random_state=0),\n",
    "        'params': {\n",
    "            'C': [0.1, 1, 10, 50, 100],  # Rango m√°s amplio para regularizaci√≥n\n",
    "            'solver': ['liblinear', 'lbfgs'],\n",
    "            'penalty': ['l1', 'l2'] # Probar regularizaci√≥n L1 y L2\n",
    "        }\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(random_state=0),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200, 300], # M√°s √°rboles para mayor estabilidad\n",
    "            'max_depth': [10, 20, 30, None], # Mayor profundidad\n",
    "            'min_samples_split': [2, 5, 10], # Control de sobreajuste\n",
    "            'min_samples_leaf': [1, 2, 4],   # Control de sobreajuste\n",
    "            'class_weight': [None, 'balanced']\n",
    "        }\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model': SVC(random_state=0),\n",
    "        'params': {\n",
    "            'C': [0.1, 1, 10, 100], # Rango m√°s amplio para regularizaci√≥n\n",
    "            'kernel': ['linear', 'rbf'],\n",
    "            'gamma': ['scale', 'auto', 0.1, 1], # Clave para el kernel RBF\n",
    "            'class_weight': [None, 'balanced']\n",
    "        }\n",
    "    },\n",
    "    'NaiveBayes': {\n",
    "        'model': MultinomialNB(),\n",
    "        'params': {\n",
    "            'alpha': [0.01, 0.1, 0.5, 1.0, 2.0] # Ajuste m√°s fino del suavizado\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# EXPERIMENTACI√ìN CON VALIDACI√ìN CRUZADA (5 FOLDS)\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_model_with_cv(model, X, y, balance_strategy=None):\n",
    "    \"\"\"Eval√∫a modelo con 5-fold CV y estrategia de balanceo\"\"\"\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    \n",
    "    if balance_strategy:\n",
    "        # Crear pipeline con balanceo\n",
    "        pipeline = Pipeline([\n",
    "            ('sampler', balance_strategy),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "    else:\n",
    "        pipeline = Pipeline([\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "    \n",
    "    cv_results = cross_validate(\n",
    "        pipeline, X, y, \n",
    "        cv=kf, \n",
    "        scoring='f1_macro',\n",
    "        n_jobs=-1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'mean_train_f1': np.mean(cv_results['train_score']),\n",
    "        'std_train_f1': np.std(cv_results['train_score']),\n",
    "        'mean_test_f1': np.mean(cv_results['test_f1_macro']),\n",
    "        'std_test_f1': np.std(cv_results['test_f1_macro'])\n",
    "    }\n",
    "\n",
    "# Almacenar resultados\n",
    "results = []\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMIENZO DE EXPERIMENTACI√ìN CON 5-FOLD CV\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Probar diferentes combinaciones\n",
    "vectorizers = create_vectorizers()\n",
    "lexicon_sel = load_sel()\n",
    "\n",
    "for vec_name, vectorizer in vectorizers.items():\n",
    "    print(f\"\\n--- Probando representaci√≥n: {vec_name} ---\")\n",
    "    \n",
    "    # Vectorizar texto\n",
    "    X_train_vec = vectorizer.fit_transform(X_train_text)\n",
    "    X_test_vec = vectorizer.transform(X_test_text)\n",
    "    \n",
    "    # Obtener caracter√≠sticas SEL\n",
    "    if lexicon_sel:\n",
    "        sel_features_train = getSELFeatures(X_train_text, lexicon_sel)\n",
    "        sel_features_test = getSELFeatures(X_test_text, lexicon_sel)\n",
    "        sel_train_array = np.array([[f[col] for col in sel_cols] for f in sel_features_train])\n",
    "        sel_test_array = np.array([[f[col] for col in sel_cols] for f in sel_features_test])\n",
    "        \n",
    "        # Combinar caracter√≠sticas\n",
    "        X_train_combined = hstack([X_train_vec, csr_matrix(sel_train_array)])\n",
    "        X_test_combined = hstack([X_test_vec, csr_matrix(sel_test_array)])\n",
    "    else:\n",
    "        X_train_combined = X_train_vec\n",
    "        X_test_combined = X_test_vec\n",
    "    \n",
    "    for model_name, model_config in models_config.items():\n",
    "        for balance_name, balance_strategy in balance_strategies.items():\n",
    "            print(f\"  Modelo: {model_name} | Balanceo: {balance_name}\")\n",
    "            \n",
    "            try:\n",
    "                # Evaluar con CV\n",
    "                cv_results = evaluate_model_with_cv(\n",
    "                    model_config['model'], \n",
    "                    X_train_combined, \n",
    "                    y_train, \n",
    "                    balance_strategy\n",
    "                )\n",
    "                \n",
    "                # Guardar resultados\n",
    "                result = {\n",
    "                    'representation': vec_name,\n",
    "                    'model': model_name,\n",
    "                    'balancing': balance_name,\n",
    "                    'mean_train_f1': cv_results['mean_train_f1'],\n",
    "                    'std_train_f1': cv_results['std_train_f1'],\n",
    "                    'mean_test_f1': cv_results['mean_test_f1'],\n",
    "                    'std_test_f1': cv_results['std_test_f1']\n",
    "                }\n",
    "                results.append(result)\n",
    "                \n",
    "                print(f\"    F1-macro: {cv_results['mean_test_f1']:.4f} ¬± {cv_results['std_test_f1']:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error: {e}\")\n",
    "                continue\n",
    "\n",
    "# =============================================================================\n",
    "# AN√ÅLISIS DE RESULTADOS Y SELECCI√ìN DEL MEJOR MODELO\n",
    "# =============================================================================\n",
    "\n",
    "# Crear DataFrame con resultados\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESULTADOS COMPLETOS DE EXPERIMENTACI√ìN\")\n",
    "print(\"=\" * 80)\n",
    "print(results_df.round(4))\n",
    "\n",
    "# Encontrar mejor combinaci√≥n\n",
    "best_result_idx = results_df['mean_test_f1'].idxmax()\n",
    "best_result = results_df.loc[best_result_idx]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MEJOR COMBINACI√ìN ENCONTRADA\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Representaci√≥n: {best_result['representation']}\")\n",
    "print(f\"Modelo: {best_result['model']}\")\n",
    "print(f\"Balanceo: {best_result['balancing']}\")\n",
    "print(f\"F1-macro (CV): {best_result['mean_test_f1']:.4f} ¬± {best_result['std_test_f1']:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ENTRENAMIENTO FINAL DEL MEJOR MODELO\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ENTRENAMIENTO FINAL DEL MEJOR MODELO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Recrear la mejor representaci√≥n\n",
    "best_vectorizer = vectorizers[best_result['representation']]\n",
    "X_train_vec = best_vectorizer.fit_transform(X_train_text)\n",
    "X_test_vec = best_vectorizer.transform(X_test_text)\n",
    "\n",
    "# Agregar caracter√≠sticas SEL si est√°n disponibles\n",
    "if lexicon_sel:\n",
    "    sel_features_train = getSELFeatures(X_train_text, lexicon_sel)\n",
    "    sel_features_test = getSELFeatures(X_test_text, lexicon_sel)\n",
    "    sel_train_array = np.array([[f[col] for col in sel_cols] for f in sel_features_train])\n",
    "    sel_test_array = np.array([[f[col] for col in sel_cols] for f in sel_features_test])\n",
    "    X_train_final = hstack([X_train_vec, csr_matrix(sel_train_array)])\n",
    "    X_test_final = hstack([X_test_vec, csr_matrix(sel_test_array)])\n",
    "else:\n",
    "    X_train_final = X_train_vec\n",
    "    X_test_final = X_test_vec\n",
    "\n",
    "# Configurar el mejor modelo con balanceo\n",
    "best_model_config = models_config[best_result['model']]\n",
    "best_balance_strategy = balance_strategies[best_result['balancing']]\n",
    "\n",
    "if best_balance_strategy:\n",
    "    final_model = Pipeline([\n",
    "        ('sampler', best_balance_strategy),\n",
    "        ('classifier', best_model_config['model'])\n",
    "    ])\n",
    "else:\n",
    "    final_model = Pipeline([\n",
    "        ('classifier', best_model_config['model'])\n",
    "    ])\n",
    "\n",
    "# Entrenar modelo final\n",
    "final_model.fit(X_train_final, y_train)\n",
    "\n",
    "# Predicciones finales\n",
    "y_pred = final_model.predict(X_test_final)\n",
    "\n",
    "# M√©tricas finales\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "test_f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "test_f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"\\n=== RESULTADOS FINALES EN TEST ===\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"F1-macro: {test_f1_macro:.4f}\")\n",
    "print(f\"F1-weighted: {test_f1_weighted:.4f}\")\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "print(f\"\\nMatriz de confusi√≥n:\")\n",
    "print(confusion_matrix(y_test, y_pred, labels=[1, 2, 3, 4, 5]))\n",
    "\n",
    "# Reporte de clasificaci√≥n detallado\n",
    "target_names = ['1', '2', '3', '4', '5']\n",
    "print(f\"\\nReporte de clasificaci√≥n:\")\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "# =============================================================================\n",
    "# TABLA RESUMEN Y JUSTIFICACI√ìN\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TABLA RESUMEN DE MEJORES RESULTADOS POR COMBINACI√ìN\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Top 10 mejores resultados\n",
    "top_results = results_df.nlargest(10, 'mean_test_f1')[['representation', 'model', 'balancing', 'mean_test_f1', 'std_test_f1']]\n",
    "print(top_results.round(4))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"JUSTIFICACI√ìN DE LA ELECCI√ìN DEL MODELO FINAL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"Se seleccion√≥ la combinaci√≥n:\")\n",
    "print(f\"‚Ä¢ Representaci√≥n: {best_result['representation']}\")\n",
    "print(f\"‚Ä¢ Modelo: {best_result['model']}\")\n",
    "print(f\"‚Ä¢ Estrategia de balanceo: {best_result['balancing']}\")\n",
    "print(f\"‚Ä¢ F1-macro en validaci√≥n cruzada: {best_result['mean_test_f1']:.4f}\")\n",
    "\n",
    "justification = {\n",
    "    'CountBinary': \"Vectorizaci√≥n binaria simple que captura presencia/ausencia de t√©rminos.\",\n",
    "    'CountFreq': \"Vectorizaci√≥n por frecuencia que considera repeticiones de t√©rminos.\",\n",
    "    'TFIDF': \"TF-IDF pondera t√©rminos por importancia en el corpus.\",\n",
    "    'TFIDF_ngrams': \"TF-IDF con n-gramas captura frases y contextos locales.\",\n",
    "    'LogisticRegression': \"Modelo lineal interpretable y eficiente para clasificaci√≥n.\",\n",
    "    'RandomForest': \"Ensemble robusto que maneja bien relaciones no lineales.\",\n",
    "    'SVM': \"Buen rendimiento en espacios de alta dimensionalidad.\",\n",
    "    'NaiveBayes': \"Modelo probabil√≠stico r√°pido y eficiente para texto.\",\n",
    "    'none': \"Sin balanceo - datos originales.\",\n",
    "    'oversample': \"Oversampling de clases minoritarias.\",\n",
    "    'undersample': \"Undersampling de clases mayoritarias.\",\n",
    "    'smote': \"SMOTE genera muestras sint√©ticas de clases minoritarias.\"\n",
    "}\n",
    "\n",
    "print(f\"\\nJustificaci√≥n t√©cnica:\")\n",
    "print(f\"- {justification[best_result['representation']]}\")\n",
    "print(f\"- {justification[best_result['model']]}\")\n",
    "print(f\"- {justification[best_result['balancing']]}\")\n",
    "\n",
    "# =============================================================================\n",
    "# VERIFICACI√ìN DEL OBJETIVO Y GUARDADO\n",
    "# =============================================================================\n",
    "\n",
    "if test_f1_macro >= 0.70:\n",
    "    print(f\"\\n‚úÖ ¬°OBJETIVO CUMPLIDO! F1-macro = {test_f1_macro:.4f} >= 0.70\")\n",
    "    \n",
    "    # Guardar modelo final\n",
    "    model_info = {\n",
    "        'model': final_model,\n",
    "        'vectorizer': best_vectorizer,\n",
    "        'representation': best_result['representation'],\n",
    "        'balancing': best_result['balancing'],\n",
    "        'test_metrics': {\n",
    "            'f1_macro': test_f1_macro,\n",
    "            'f1_weighted': test_f1_weighted,\n",
    "            'accuracy': test_accuracy\n",
    "        },\n",
    "        'feature_names': best_vectorizer.get_feature_names_out().tolist() + (sel_cols if lexicon_sel else [])\n",
    "    }\n",
    "    \n",
    "    model_filename = f'best_model_{best_result[\"model\"]}_{best_result[\"representation\"]}_{best_result[\"balancing\"]}_f1_{test_f1_macro:.4f}.pkl'\n",
    "    with open(model_filename, 'wb') as f:\n",
    "        pickle.dump(model_info, f)\n",
    "    print(f\"Modelo guardado como: {model_filename}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n‚ùå OBJETIVO NO CUMPLIDO: F1-macro = {test_f1_macro:.4f} < 0.70\")\n",
    "    print(\"Recomendaciones:\")\n",
    "    print(\"1. Probar t√©cnicas m√°s avanzadas de balanceo\")\n",
    "    print(\"2. Aumentar caracter√≠sticas l√©xicas y de sentimiento\")\n",
    "    print(\"3. Probar modelos ensemble m√°s complejos\")\n",
    "    print(\"4. Mejorar el preprocesamiento de texto\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXPERIMENTACI√ìN COMPLETADA\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
